/**************************************************************************/
/*                                                                        */
/*  Copyright (C) 2004-2019 INTERSEC SA                                   */
/*                                                                        */
/*  Should you receive a copy of this source code, you must check you     */
/*  have a proper, written authorization of INTERSEC to hold it. If you   */
/*  don't have such an authorization, you must DELETE all source code     */
/*  files in your possession, and inform INTERSEC of the fact you obtain  */
/*  these files. Should you not comply to these terms, you can be         */
/*  prosecuted in the extent permitted by applicable law.                 */
/*                                                                        */
/**************************************************************************/

#include <lib-common/arith.h>
#include <lib-common/datetime.h>
#include <lib-common/el.h>
#include <lib-common/unix.h>
#include "thr.h"

#if !defined(__x86_64__) && !defined(__i386__)
#  error "this file assumes a strict memory model and is probably buggy on !x86"
#endif

#if !defined(NDEBUG) && !defined(__has_tsan)
# define __has_thr_acc
#endif

typedef struct thr_qnode_t thr_qnode_t;
struct thr_qnode_t {
    mpsc_node_t  qnode;
    thr_job_t   *job;
    thr_syn_t   *syn;
} __attribute__((aligned(CACHE_LINE_SIZE)));

struct thr_queue_t {
    thr_job_t    run;
    thr_job_t    destroy;
    mpsc_queue_t q;
    _Atomic(ssize_t) running_on;
} __attribute__((aligned(CACHE_LINE_SIZE)));

typedef _Atomic(thr_job_t *) atomic_thr_job_t;

typedef struct thr_info_t {
    /** top of the deque.
     * this variable is accessed through shared_read, and modified through
     * atomic_bool_cas which ensure load/store consistency. Hence no barrier
     * is needed to read it.
     */
    atomic_uint top;
    /** bottom of the deque.
     * this variable is only modified by the current thread, hence it doesn't
     * need read barriers to read it. Though other threads do need a read
     * barrier before acces, and the owner of the deque must publish new
     * values through a write barrier.
     */
    atomic_uint bot;
    atomic_bool alive;
    pthread_t  thr;
    char       padding_0[CACHE_LINE_SIZE];

    struct deque_entry {
        atomic_thr_job_t job;
        thr_syn_t *syn;
    } q[THR_JOB_MAX];
    char       padding_1[CACHE_LINE_SIZE];

#define NCACHE_MAX    1024
    size_t       ncache_sz;
    thr_qnode_t  ncache;

#ifdef __has_thr_acc
    struct thr_acc {
        uint64_t time;
        uint64_t ec_wait_time;
        uint64_t ec_steal_time;

        unsigned jobs_local;
        unsigned jobs_queued;
        unsigned jobs_run;
        unsigned ec_gets;
        unsigned ec_waits;
        unsigned jobs_steals;
        unsigned jobs_stealed;
        unsigned jobs_failed_steals;
        unsigned jobs_failed_dequeues;
    } acc;
#endif
} thr_info_t;

static struct {
    atomic_bool       stopping;
    thr_evc_t         ec;
    thr_evc_t         start_bar_main;
    thr_evc_t         start_bar_thr;

    thr_info_t       *threads;
    el_t              before;
    el_t              wakeel;
    thr_queue_t       main_queue;
    uint64_t          reset_time;
    proctimer_t       st;
} thr_job_g;
#define _G  thr_job_g

static __thread int self_tid_g = (-1);
static __thread thr_info_t *self_g;

size_t thr_parallelism_g;
thr_queue_t *const thr_queue_main_g = &_G.main_queue;

typedef _Atomic(thr_evc_t *) atomic_thr_evc_t;
static atomic_thr_evc_t thr0_cur_ec_g;
static bool reload_at_fork_g;

/* Tracing {{{ */
#ifdef __has_thr_acc

void thr_acc_reset(void)
{
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        p_clear(&_G.threads[i].acc, 1);
    }
    _G.reset_time = hardclock();
    proctimer_start(&_G.st);
}

static size_t int_width(uint64_t i)
{
    int w = 1;

    for (; i > 1; i /= 10)
        w++;
    return w;
}

void thr_acc_set_affinity(size_t offs)
{
#if defined(CPU_SET)
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        cpu_set_t set;

        CPU_ZERO(&set);
        CPU_SET((i + offs) % thr_parallelism_g, &set);
        if (pthread_setaffinity_np(_G.threads[i].thr, sizeof(set), &set))
            e_panic("cannot set affinity");
    }
#endif
}

void thr_acc_trace(int lvl, const char *fmt, ...)
{
    uint64_t avg, wall = hardclock() - _G.reset_time;
    unsigned waste, speedup;

    struct thr_acc total = { .time = 0, };
    struct thr_acc width = { .time = 0, };
    va_list  ap;
    SB_8k(sb);

    proctimer_stop(&_G.st);

    va_start(ap, fmt);
    sb_addvf(&sb, fmt, ap);
    va_end(ap);

    width.time = int_width(wall / 1000000);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        struct thr_acc *acc = &_G.threads[i].acc;

        total.time        += acc->time;
        total.jobs_local  += acc->jobs_local;
        total.jobs_queued += acc->jobs_queued;
        total.jobs_run    += acc->jobs_run;
        total.ec_gets     += acc->ec_gets;
        total.ec_waits    += acc->ec_waits;
        total.ec_wait_time += acc->ec_wait_time;
        total.jobs_steals   += acc->jobs_steals;
        total.jobs_stealed  += acc->jobs_stealed;
        total.jobs_failed_steals += acc->jobs_failed_steals;
        total.jobs_failed_dequeues += acc->jobs_failed_dequeues;

        width.time         = MAX(width.time,        int_width(acc->time / 1000000));
        width.jobs_local   = MAX(width.jobs_local,  int_width(acc->jobs_local));
        width.jobs_queued  = MAX(width.jobs_queued, int_width(acc->jobs_queued));
        width.jobs_run     = MAX(width.jobs_run,    int_width(acc->jobs_run));
        width.ec_gets      = MAX(width.ec_gets,     int_width(acc->ec_gets));
        width.ec_waits     = MAX(width.ec_waits,    int_width(acc->ec_waits));
        width.ec_wait_time = MAX(width.ec_wait_time, int_width(acc->ec_wait_time / 1000000));
        width.jobs_steals    = MAX(width.jobs_steals,   int_width(acc->jobs_steals));
        width.jobs_stealed   = MAX(width.jobs_stealed,   int_width(acc->jobs_stealed));
        width.jobs_failed_steals = MAX(width.jobs_failed_steals,
                                       int_width(acc->jobs_failed_steals));
        width.jobs_failed_dequeues = MAX(width.jobs_failed_dequeues,
                                       int_width(acc->jobs_failed_dequeues));
    }

    e_trace(lvl, "----- %*pM", sb.len, sb.data);

    if (total.jobs_run == 0) {
        e_trace(lvl, "----- No jobs since last reset");
        return;
    }

#define TIME_FMT_ARG(t)   (int)width.time, (int)((t) / 1000000)

    sb_reset(&sb);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        struct thr_acc *acc = &_G.threads[i].acc;

        e_trace(lvl, " %2zd: %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
                "%*u failed dequeues, %*u failed queues, %*u gets, %*u waits (%*uM)", i,
                TIME_FMT_ARG(acc->time),
                width.jobs_queued, acc->jobs_queued,
                width.jobs_run,    acc->jobs_run,
                width.jobs_steals, acc->jobs_steals,
                width.jobs_stealed, acc->jobs_stealed,
                width.jobs_failed_steals, acc->jobs_failed_steals,
                width.jobs_failed_dequeues, acc->jobs_failed_dequeues,
                width.jobs_local, acc->jobs_local,
                width.ec_gets,     acc->ec_gets,
                width.ec_waits,    acc->ec_waits,
                TIME_FMT_ARG(acc->ec_wait_time));
    }
    e_trace(lvl, "wall %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
            "%*u failed dequeues, %*u failed queues, %*u gets, %*u waits (%*uM)", TIME_FMT_ARG(wall),
            width.jobs_queued, total.jobs_queued,
            width.jobs_run,    total.jobs_run,
            width.jobs_steals, total.jobs_steals,
            width.jobs_stealed, total.jobs_stealed,
            width.jobs_failed_steals, total.jobs_failed_steals,
            width.jobs_failed_dequeues, total.jobs_failed_dequeues,
            width.jobs_local, total.jobs_local,
            width.ec_gets,     total.ec_gets,
            width.ec_waits,    total.ec_waits,
            TIME_FMT_ARG(total.ec_wait_time));
    avg     = (uint64_t)(total.time / thr_parallelism_g);
    waste   = (uint64_t)(wall - avg) * 10000 / wall;
    speedup = total.time * 100 / wall;
    e_trace(lvl, "avg  %*uM (%d.%02d%% waste, %d.%02dx speedup)",
            TIME_FMT_ARG(avg), waste / 100, waste % 100,
            speedup / 100, speedup % 100);
    e_trace(lvl, "job  %*jd cycles in avg",
            (int)width.time, total.time / total.jobs_run);
    e_trace(lvl, "cost %*jd cycles of overhead per job", (int)width.time,
            (wall * thr_parallelism_g - total.time) / total.jobs_run);
    e_trace(lvl, "     %s", proctimer_report(&_G.st, NULL));
#undef TIME_FMT_ARG
}

#endif
/* }}} */
/* atomic dequeue {{{ */

static bool job_run(thr_job_t * nonnull job, thr_syn_t *syn)
{
#ifdef __has_thr_acc
    unsigned long start = hardclock();

    self_g->acc.jobs_run++;
#endif

    if ((uintptr_t)job & 3) {
        block_t blk = (block_t)((uintptr_t)job & ~(uintptr_t)3);

        blk();
        if ((uintptr_t)job & 1)
            Block_release(blk);
    } else {
        (*job->run)(job, syn);
    }

#ifdef __has_thr_acc
    self_g->acc.time += (hardclock() - start);
#endif
    if (syn)
        thr_syn__job_done(syn);
    return true;
}

void thr_syn_schedule(thr_syn_t *syn, thr_job_t *job)
{
    unsigned bot, top;
    struct deque_entry *e;

    if (syn)
        thr_syn__job_prepare(syn);

    /* Read the current limits of the queue. Since 'bot' cannot be changed by
     * another thread and 'top' can only be moved in the direction of
     * consuming threads, 'bot' is effectively the position where the job
     * should be inserted and 'bot - top' gives an upper bound to the number of
     * job in the queue (and is the actual number if no other thread try to
     * steal a job to that one concurrently to the insertion).
     */
    bot = atomic_load(&self_g->bot);
    top = atomic_load(&self_g->top);

    /* Looks like there may be too many jobs in the queue of that thread, run
     * the new one immediately.
     */
    if (unlikely((int)(bot - top) >= THR_JOB_MAX)) {
#ifdef __has_thr_acc
        self_g->acc.jobs_local++;
#endif
        job_run(job, syn);
        return;
    }

    e = &self_g->q[bot % THR_JOB_MAX];

    /* Looks like we are inserting the job in an empty queue and that the
     * current object is still used by another thread. Run it locally.
     */
    if (atomic_load_explicit(&e->job, memory_order_acquire) != NULL) {
#ifdef __has_thr_acc
        self_g->acc.jobs_local++;
#endif
        job_run(job, syn);
        return;
    }

    /* Add the job in the queue and update bottom. Since other threads can
     * only consume jobs from the queue (increment top), we can safely add the
     * new job at q[bot] and then increment bot
     */
    e->syn = syn;
    atomic_store_explicit(&e->job, job, memory_order_release);
    atomic_store(&self_g->bot, bot + 1);

#ifdef __has_thr_acc
    self_g->acc.jobs_queued++;
#endif


    thr_ec_signal(&_G.ec);
}

void thr_schedule(thr_job_t *job)
{
    thr_syn_schedule(NULL, job);
}


/** Consume the top job of the specified thread.
 *
 * @param ti    the thread info structure of the thread to update
 * @param top__ expected value of the 'top' line.
 * @return true in case of success, false if another thread already fetched
 *         the top element.
 */
static bool thr_consume_top(thr_info_t *ti, unsigned top, unsigned count)
{
    return atomic_compare_exchange_strong_explicit(&ti->top, &top,
                                                   top + count,
                                                   memory_order_relaxed,
                                                   memory_order_relaxed);
}

static bool thr_run_deque_entry(struct deque_entry *e)
{
    thr_job_t *job = atomic_load_explicit(&e->job, memory_order_acquire);
    thr_syn_t *syn = e->syn;

    atomic_store_explicit(&e->job, NULL, memory_order_release);
    return job_run(job, syn);
}

/** Run the 'bottom' job of the queue of the current thread.
 *
 * @return true if a job has been run, false if the queue is empty.
 */
static bool thr_job_dequeue(void)
{
    unsigned top, bot;

    /* Read the bottom job and update the mark to mark that job as consumed.
     * The remaining of the function will ensure we were effectively the first
     * to reclaim the ownership of that job.
     */
    bot = atomic_load_explicit(&self_g->bot, memory_order_relaxed) - 1;
    atomic_store_explicit(&self_g->bot, bot, memory_order_seq_cst);
    atomic_thread_fence(memory_order_acq_rel);

    /* Read the top line. The memory barrier ensure that the read is effectively
     * done after updating bot and thus that other thread may have seen the
     * update of bot before touching top.
     */
    top = atomic_load_explicit(&self_g->top, memory_order_relaxed);

    /* There are remaining jobs after moving the bottom line. Top has been
     * read after bot, and since bot is refetched before each individual
     * trysteal,  we're sure 'top' cannot have reached 'bot', and thus
     * we are the owner of the job we fetched, run it.
     */
    if (likely((int)(bot - top) > 0)) {
        return thr_run_deque_entry(&self_g->q[bot % THR_JOB_MAX]);
    }

    /* 'bot' and 'top' are equal, that mean that either we're consuming the
     * last job of the queue or another thread did it. To know if the job
     * is for us, check that top hasn't been moved. If it has been moved, the
     * CAS will fail and the job is not for us, if it hasn't been moved, then
     * move it to ensure nobody else will do it, then, move back 'bot' and
     * 'top' to the same point since the queue is empty.
     */
    if (likely(bot == top)) {
        if (likely(thr_consume_top(self_g, top, 1))) {
            atomic_store_explicit(&self_g->bot, top + 1, memory_order_relaxed);
            return thr_run_deque_entry(&self_g->q[bot % THR_JOB_MAX]);
        } else {
#ifdef __has_thr_acc
            self_g->acc.jobs_failed_dequeues++;
#endif
        }
    }

    /* 'top' has been moved to the previous value of bot, this means the job
     * has already been run by another thread. Since top > bot, we're
     * sure nobody else will try to read another job and that
     * top == bot + 1 == previous value of bot. Thus, we have to restore 'bot'
     * to its previous value since we didn't get ownership of the job.
     */
    atomic_store_explicit(&self_g->bot, bot + 1, memory_order_relaxed);
    return false;
}

/** Try to steal a job from the queue of another thread.
 *
 * @param ti  The thread info structure of another thread.
 * @return 1 if a job has been stolen, 0 if the thread's queue is empty,
 *         -1 if the attempt failed (a retry may be needed)
 */
static int thr_job_try_steal(thr_info_t *ti, int depth)
{
    unsigned top, bot;

    /* Read the limits of the queue of the thread and fetch the top 'job' of
     * the queue.
     */
    top = atomic_load_explicit(&ti->top, memory_order_relaxed);
    bot = atomic_load_explicit(&ti->bot, memory_order_relaxed);

    /* If the queue does not seem to be empty, then we know we own the job if
     * and only if we can CAS top. This works because a concurrent
     * dequeue/steal will always CAS top when consuming the last job of the
     * queue. The tricky part is the concurrency between dequeue and steal
     * solved by the fact we'll always move 'bot' and 'top' in dequeue when
     * emptying the queue.
     */
    if ((int)(bot - top) > 0) {
        if (likely(thr_consume_top(ti, top, 1))) {
#ifdef __has_thr_acc
            self_g->acc.jobs_stealed += 1;
            self_g->acc.jobs_steals++;

#endif
            return thr_run_deque_entry(&ti->q[top % THR_JOB_MAX]);
        } else {
#ifdef __has_thr_acc
            self_g->acc.jobs_failed_steals++;
#endif
            return -1;
        }
    }
    return 0;
}

#undef cas_top

/* FIXME: optimize for large number of threads, with a loopless fastpath */
static int thr_job_steal(void)
{
    bool empty = true;

    for (size_t i = 1; i < thr_parallelism_g; i++) {
        size_t tid = (i + self_tid_g) % thr_parallelism_g;
        int res;

        res = thr_job_try_steal(&_G.threads[tid], i);
        if (res > 0) {
            return 1;
        } else
        if (res < 0) {
            empty = false;
        }
        sched_yield();
    }
    return empty ? 0 : -1;
}

/* }}} */
/* serial queues {{{ */

static ALWAYS_INLINE thr_qnode_t *thr_qnode_of(mpsc_node_t *node)
{
    return container_of(node, thr_qnode_t, qnode);
}

static thr_qnode_t *thr_qnode_create(void)
{
    if (likely(self_g)) {
        mpsc_node_t *m = atomic_load_explicit(&self_g->ncache.qnode.next,
                                              memory_order_relaxed);

        if (likely(m)) {
            thr_qnode_t *n = thr_qnode_of(m);

            atomic_store_explicit(&self_g->ncache.qnode.next,
                                  atomic_load_explicit(&n->qnode.next,
                                                       memory_order_relaxed),
                                  memory_order_relaxed);
            self_g->ncache_sz--;
            return n;
        }
    }
    return p_new_raw(thr_qnode_t, 1);
}

static void thr_qnode_destroy(mpsc_node_t *n)
{
    if (likely(self_g && self_g->ncache_sz < NCACHE_MAX)) {
        self_g->ncache_sz++;
        atomic_store_explicit(&n->next,
                              atomic_load_explicit(&self_g->ncache.qnode.next,
                                                   memory_order_relaxed),
                              memory_order_relaxed);
        atomic_store_explicit(&self_g->ncache.qnode.next, n,
                              memory_order_relaxed);
    } else {
        free(thr_qnode_of(n));
    }
}

static void thr_queue_wipe(thr_queue_t *q)
{
    assert (mpsc_queue_looks_empty(&q->q));
}
GENERIC_DELETE(thr_queue_t, thr_queue);

static void thr_queue_run_node(mpsc_node_t *m, data_t data)
{
    thr_qnode_t *n = thr_qnode_of(m);
    thr_job_t *job = n->job;
    thr_syn_t *syn = n->syn;

    thr_qnode_destroy(m);
    job_run(job, syn);
}

#define THR_QUEUE_NOT_RUNNING  -2

static void thr_queue_drain(thr_queue_t *q)
{
    bool wipe = false;
    mpsc_it_t it;
    ssize_t id;

    if (unlikely(mpsc_queue_looks_empty(&q->q))) {
        /* The queue should not be marked empty *and* scheduled. the only
         * queue that should go through this slowpath is the main queue
         * since it's not really scheduled like the other ones
         */
        assert (q == thr_queue_main_g);
        return;
    }

    id = thr_id();
    atomic_store(&q->running_on, id);
    mpsc_queue_drain_start(&it, &q->q);
    do {
        mpsc_node_t *m = mpsc_queue_drain_fast(&it, &thr_queue_run_node,
                                               (data_t){ .ptr = NULL });
        thr_qnode_t *n = thr_qnode_of(m);

        job_run(n->job, n->syn);
        wipe = (n->job == &q->destroy);
    } while (!mpsc_queue_drain_end(&it, &thr_qnode_destroy));
    atomic_compare_exchange_strong(&q->running_on, &id,
                                   THR_QUEUE_NOT_RUNNING);

    if (wipe) {
        thr_queue_delete(&q);
    }
}

static void thr_queue_run(thr_job_t *job, thr_syn_t *syn)
{
    thr_queue_drain(container_of(job, thr_queue_t, run));
}

static void thr_queue_finalize(thr_job_t *job, thr_syn_t *syn)
{
    /*
     * Do nothing in the finalize, thr_queue_drain will perform the deletion
     * itself. This function serves as a marker that we reached the end of the
     * queue.
     */
}

static thr_queue_t *thr_queue_init(thr_queue_t *q)
{
    p_clear(q, 1);
    mpsc_queue_init(&q->q);
    q->run.run     = &thr_queue_run;
    q->destroy.run = &thr_queue_finalize;
    atomic_init(&q->running_on, THR_QUEUE_NOT_RUNNING);
    return q;
}

static void thr_wakeup_thr0(void)
{
    if (self_tid_g != 0) {
        thr_evc_t *ec = atomic_exchange(&thr0_cur_ec_g, NULL);

        if (ec) {
            thr_ec_broadcast(ec);
            atomic_store(&thr0_cur_ec_g, ec);
        } else {
            el_wake_fire(_G.wakeel);
        }
    }
}

void thr_syn_queue(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    if (likely(q)) {
        thr_qnode_t *n = thr_qnode_create();

        n->job = job;
        n->syn = syn;
        if (syn)
            thr_syn__job_prepare(syn);

        if (mpsc_queue_push(&q->q, &n->qnode)) {
            if (q == thr_queue_main_g) {
                thr_wakeup_thr0();
            } else {
                thr_schedule(&q->run);
            }
        }
    } else {
        thr_syn_schedule(syn, job);
    }
}

void thr_queue_sync(thr_queue_t *q, thr_job_t *job)
{
    thr_qnode_t *n;
    thr_syn_t syn;

    if (thr_is_on_queue(q)) {
        /* We're already on the queue, so don't pass the queue again */
        job_run(job, NULL);
        return;
    }

    thr_syn_init(&syn);
    thr_syn__job_prepare(&syn);
    n = thr_qnode_create();
    n->job = job;
    n->syn = &syn;
    /*
     * if mpsc_queue_push returns 1, then we're alone !
     */
    if (mpsc_queue_push(&q->q, &n->qnode)) {
        if (q == thr_queue_main_g && self_tid_g != 0) {
            thr_wakeup_thr0();
        } else {
            thr_queue_drain(q);
            return;
        }
    }
    thr_syn_wait(&syn);
    thr_syn_wipe(&syn);
}

void thr_queue(thr_queue_t *q, thr_job_t *job)
{
    thr_syn_queue(NULL, q, job);
}

thr_queue_t *thr_queue_create(void)
{
    thr_queue_t *q = p_new(thr_queue_t, 1);

    return thr_queue_init(q);
}

void thr_queue_destroy(thr_queue_t *q, bool wait)
{
    assert (q != thr_queue_main_g);
    if (wait) {
        thr_queue_sync(q, &q->destroy);
    } else {
        thr_queue(q, &q->destroy);
    }
}

bool thr_is_on_queue(thr_queue_t *q)
{
    return atomic_load(&q->running_on) == (ssize_t)thr_id()
        || (q == thr_queue_main_g && thr_id() == 0);
}

/* }}} */
/* thread run loop {{{ */

static void thr_info_init(int tid)
{
    uint64_t key = thr_ec_get(&thr_job_g.start_bar_thr);

    self_tid_g  = tid;
    self_g      = _G.threads + tid;
    self_g->thr = pthread_self();
    atomic_thread_fence(memory_order_acq_rel);
    atomic_store(&self_g->alive, true);
    thr_ec_signal(&thr_job_g.start_bar_main);
    if (tid != 0) {
        thr_ec_wait(&thr_job_g.start_bar_thr, key);
    }
    atomic_thread_fence(memory_order_acq_rel);
}

static void thr_info_cleanup(void *unused)
{
    mpsc_node_t *m;

    do {
        if (self_tid_g == 0)
            thr_queue_drain(thr_queue_main_g);
    } while (thr_job_dequeue());
    if (self_tid_g == 0)
        thr_queue_wipe(thr_queue_main_g);
    while ((m = atomic_load_explicit(&self_g->ncache.qnode.next,
                                     memory_order_relaxed)))
    {
        thr_qnode_t *n = thr_qnode_of(m);

        self_g->ncache.qnode.next = n->qnode.next;
        free(n);
    }
    atomic_thread_fence(memory_order_acq_rel);
    atomic_store(&self_g->alive, false);
}

static void *thr_job_main(void *arg)
{
    sigset_t set;

    sigemptyset(&set);
    sigaddset(&set, SIGABRT);
    sigaddset(&set, SIGILL);
    sigaddset(&set, SIGFPE);
    sigaddset(&set, SIGSEGV);
    sigaddset(&set, SIGBUS);
#if defined(__linux__)
    sigaddset(&set, SIGSTKFLT);
#endif
    pthread_sigmask(SIG_UNBLOCK, &set, NULL);

    thr_info_init((uintptr_t)arg);
    pthread_cleanup_push(thr_info_cleanup, NULL);

    while (likely(!atomic_load(&_G.stopping))) {
        int res;
        uint64_t key = 0;

        while (likely(thr_job_dequeue())) {
            continue;
        }
        if (thr_job_steal() > 0) {
            continue;
        }

        do {
            sched_yield();
            key = thr_ec_get(&_G.ec);
#ifdef __has_thr_acc
            self_g->acc.ec_gets++;
#endif
        } while ((res = thr_job_steal()) < 0);
        if (res == 0 && !atomic_load(&_G.stopping)) {
#ifdef __has_thr_acc
            unsigned long start = hardclock();

            self_g->acc.ec_waits++;
#endif
            thr_ec_wait(&_G.ec, key);

#ifdef __has_thr_acc
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
        }
    }
    pthread_cleanup_pop(1);
    return NULL;
}

/* }}} */
/* Module init / shutdown {{{ */

bool thr_job_reload_at_fork(bool enabled)
{
    bool prev = reload_at_fork_g;

    reload_at_fork_g = enabled;

    return prev;
}

__attr_notsan__
static void thr_wipe(void)
{
    el_unregister(&_G.before);
    el_unregister(&_G.wakeel);
    p_delete(&_G.threads);
    p_clear(&_G, 1);
    thr_parallelism_g = 0;
}


static void thr_on_el(el_t ev, data_t priv)
{
    thr_queue_drain(thr_queue_main_g);
}

void thr_queue_main_drain(void)
{
    assert (self_tid_g == 0);
    thr_queue_drain(thr_queue_main_g);
    thr_ec_signal(&_G.ec);
}

static int thr_initialize(void *arg)
{
    uint64_t key;
    sigset_t fillset;
    sigset_t old;
    pthread_t tid;
    pthread_attr_t attr;

    if (!thr_parallelism_g) {
        const char *env = getenv("THR_MAX_PARALLELISM");
        size_t nb_cpu = sysconf(_SC_NPROCESSORS_CONF);

        if (env && *env) {
            thr_parallelism_g = atoi(env);
            if (thr_parallelism_g < 2) {
                e_fatal("invalid THR_PARALLELISM value `%s`", env);
            }
            thr_parallelism_g = MIN(thr_parallelism_g, nb_cpu);
        } else {
            thr_parallelism_g = nb_cpu;
        }
        thr_parallelism_g = MAX(thr_parallelism_g, 2);

        if (unlikely(mem_tool_is_running(MEM_TOOL_VALGRIND)))
            thr_parallelism_g = MIN(2, thr_parallelism_g);
        _G.threads = p_new(thr_info_t, thr_parallelism_g);

        atomic_init(&_G.stopping, false);
        for (size_t i = 0; i < thr_parallelism_g; i++) {
            atomic_init(&_G.threads[i].top, 0);
            atomic_init(&_G.threads[i].bot, 0);
            atomic_init(&_G.threads[i].alive, false);
        }

        pthread_attr_init(&attr);
        pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
        sigfillset(&fillset);
        pthread_sigmask(SIG_SETMASK, &fillset, &old);
        thr_ec_init(&thr_job_g.ec);
        thr_ec_init(&thr_job_g.start_bar_thr);
        thr_ec_init(&thr_job_g.start_bar_main);
        for (size_t i = 1; i < thr_parallelism_g; i++) {
            if (thr_create(&tid, &attr, thr_job_main, (void *)(uintptr_t)i)) {
                e_fatal("unable to create new thread: %m");
            }
        }
        thr_info_init(0);
        while ((key = thr_ec_get(&thr_job_g.start_bar_main)) != thr_parallelism_g) {
            thr_ec_wait(&thr_job_g.start_bar_main, key);
        }
        thr_ec_broadcast(&thr_job_g.start_bar_thr);
        pthread_sigmask(SIG_SETMASK, &old, NULL);
        pthread_attr_destroy(&attr);
        thr_queue_init(thr_queue_main_g);
        _G.before = el_unref(el_before_register(&thr_on_el, NULL));
        _G.wakeel = el_unref(el_wake_register(&thr_on_el, NULL));
    }

    return 0;
}

static int thr_shutdown(void)
{
    if (thr_parallelism_g && self_tid_g == 0) {
        atomic_store(&_G.stopping, true);
        thr_ec_broadcast(&_G.ec);

        for (size_t i = 1; i < thr_parallelism_g; i++) {
            atomic_thread_fence(memory_order_acq_rel);
            while (atomic_load(&_G.threads[i].alive)) {
                thr_ec_broadcast(&_G.ec);
                sched_yield();
            }
        }

        thr_info_cleanup(NULL);

        thr_wipe();
        thr_ec_wipe(&thr_job_g.start_bar_main);
        thr_ec_wipe(&thr_job_g.start_bar_thr);
        thr_ec_wipe(&thr_job_g.ec);
    }
    return 0;
}

static void thr_job_at_fork(void)
{
    thr_wipe();
    if (reload_at_fork_g) {
        thr_initialize(NULL);
    }
}

MODULE_BEGIN(thr)
    MODULE_DEPENDS_ON(thr_hooks);
    MODULE_DEPENDS_ON(el);
    MODULE_IMPLEMENTS_VOID(at_fork_on_child, &thr_job_at_fork);
MODULE_END()

/* }}} */
/* thr_syn {{{ */

size_t thr_id(void)
{
    return self_tid_g;
}

void thr_syn_wait_until(thr_syn_t *syn, bool (^cond)(void))
{
#ifdef __has_thr_acc
    unsigned long start;
#endif

    cond = ^bool (void) {
        int pending = atomic_load(&syn->pending);

        if (cond) {
            if (!cond()) {
                /* XXX ensure pending has been fetched before executing cond,
                 * otherwise this would be racy.
                 *
                 * BTW, if this assert fails, the condition probably does not
                 * really depend on the syn to be verified.
                 */
                assert (pending);
                return false;
            }
            return true;
        }
        return pending == 0;
    };

    thr_syn__retain(syn);

    if (self_tid_g == 0) {
        while (!cond()) {
            uint64_t key;
            thr_evc_t *ec;

            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                thr_queue_drain(thr_queue_main_g);
                continue;
            }
            if (likely(thr_job_dequeue())) {
                continue;
            }
            if (thr_job_steal() > 0) {
                continue;
            }

            atomic_store(&thr0_cur_ec_g, &syn->ec);
            key = thr_ec_get(&syn->ec);
#ifdef __has_thr_acc
            self_g->acc.ec_gets++;
#endif
            if (cond()) {
                goto cas;
            }
            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                goto cas;
            }

#ifdef __has_thr_acc
            self_g->acc.ec_waits++;
            start = hardclock();
#endif
            thr_ec_wait(&syn->ec, key);
#ifdef __has_thr_acc
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
          cas:
            ec = &syn->ec;
            while (unlikely(!atomic_compare_exchange_strong(&thr0_cur_ec_g,
                                                            &ec, NULL)))
            {
                sched_yield();
                ec = &syn->ec;
            }
        }
    } else {
        while (!cond()) {
            int res = -1;
            uint64_t key = 0;

            if (likely(thr_job_dequeue())) {
                continue;
            }
            if (thr_job_steal() > 0) {
                continue;
            }
            do {
                sched_yield();
                key = thr_ec_get(&syn->ec);
#ifdef __has_thr_acc
                self_g->acc.ec_gets++;
#endif
            } while (!cond() && (res = thr_job_steal()) < 0);
            if (res == 0) {
#ifdef __has_thr_acc
                self_g->acc.ec_waits++;
                start = hardclock();
#endif
                thr_ec_timedwait(&syn->ec, key, 100);
#ifdef __has_thr_acc
                self_g->acc.ec_wait_time += (hardclock() - start);
#endif
            }
        }
    }

    thr_syn__release(syn);
}

void thr_syn_wait(thr_syn_t *syn)
{
    thr_syn_wait_until(syn, NULL);
}

void thr_syn_notify(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    thr_schedule_b(^{
        thr_syn_wait(syn);
        thr_queue(q, job);
    });
}

#define TD_GET_PTR(td)  (((uintptr_t)(td)) & 0x0000ffffffffffffull)
#define TD_GET_SEQ(td)  (((uintptr_t)(td)) & 0xffff000000000000ull)

static thr_td_t *thr_syn_get_td(thr_td_t *td)
{
    return (thr_td_t *)TD_GET_PTR(td);
}

static thr_td_t *thr_syn_next_td_ptr(thr_td_t *cur, thr_td_t *next)
{
    return (thr_td_t *)((TD_GET_SEQ(cur) + 0x1000000000000ull) | TD_GET_PTR(next));
}

void thr_syn_wipe(thr_syn_t *syn)
{
    thr_td_t *ptr = thr_syn_get_td(atomic_load(&syn->head));

    thr_syn__release(syn);
    while (unlikely(atomic_load_explicit(&syn->refcnt, memory_order_acquire))) {
        cpu_relax();
    }
    thr_ec_wipe(&syn->ec);

    Block_release_p(&syn->new_td);

    while (ptr) {
        thr_td_t *next = atomic_load(&ptr->next);

        syn->delete_td(&ptr);
        ptr = next;
    }
    Block_release_p(&syn->delete_td);
}

void thr_syn_declare_td(thr_syn_t *syn, thr_td_t *(BLOCK_CARET new_td)(void),
                        void (BLOCK_CARET delete_td)(thr_td_t **))
{
    assert (!syn->new_td);
    assert (!syn->delete_td);

    syn->new_td = Block_copy(new_td);
    syn->delete_td = Block_copy(delete_td);
}

thr_td_t *thr_syn_acquire_td(thr_syn_t *syn)
{
    thr_td_t *ptr = atomic_load(&syn->head);

    assert (syn->new_td);

    if (thr_syn_get_td(ptr)) {
        thr_td_t *next;

        do {
            next = thr_syn_next_td_ptr(ptr, atomic_load(&thr_syn_get_td(ptr)->next));
        } while (!atomic_compare_exchange_weak(&syn->head, &ptr, next)
             &&  thr_syn_get_td(ptr));
    }

    ptr = thr_syn_get_td(ptr);
    if (ptr) {
        atomic_store(&ptr->next, NULL);
        return ptr;
    }

    return syn->new_td();
}

void thr_syn_release_td(thr_syn_t *syn, thr_td_t *td)
{
    thr_td_t *ptr = atomic_load(&syn->head);
    thr_td_t *next;

    do {
        next = thr_syn_next_td_ptr(ptr, td);
        atomic_store(&td->next, thr_syn_get_td(ptr));
    } while (!atomic_compare_exchange_weak(&syn->head, &ptr, next));
}

void thr_syn_collect_td(thr_syn_t *syn, void (^collector)(const thr_td_t *td))
{
    thr_td_t *ptr = thr_syn_get_td(atomic_load(&syn->head));

    assert (atomic_load(&syn->pending) == 0);

    while (ptr) {
        collector(ptr);
        ptr = atomic_load(&ptr->next);
    }
}

/* }}} */
/* {{{ High level helpers */

struct thr_for_each_lvl1_job_t {
    thr_job_t job;
    void (^blk)(size_t pos);

    size_t pos;
};

static void thr_for_each_lvl1(thr_job_t *job, thr_syn_t *syn)
{
    const struct thr_for_each_lvl1_job_t *lvl1;

    lvl1 = container_of(job, struct thr_for_each_lvl1_job_t, job);
    lvl1->blk(lvl1->pos);
}

struct thr_for_each_lvl0_job_t {
    thr_job_t job;
    void (^blk)(size_t pos);

    struct thr_for_each_lvl1_job_t *lvl1s;
    size_t from;
    size_t to;
};

static void thr_for_each_lvl0(thr_job_t *job, thr_syn_t *syn)
{
    const struct thr_for_each_lvl0_job_t *lvl0;

    lvl0 = container_of(job, struct thr_for_each_lvl0_job_t, job);

    for (size_t i = lvl0->from; i < lvl0->to; i++) {
        struct thr_for_each_lvl1_job_t *lvl1 = &lvl0->lvl1s[i - lvl0->from];

        lvl1->job.run = &thr_for_each_lvl1;
        lvl1->blk = lvl0->blk;
        lvl1->pos = i;

        thr_syn_schedule(syn, &lvl1->job);
    }
}

void thr_for_each(size_t count, void (^blk)(size_t pos))
{
    thr_syn_t syn;
    struct thr_for_each_lvl0_job_t *lvl0s = NULL;
    struct thr_for_each_lvl1_job_t *lvl1s;

    thr_syn_init(&syn);

    lvl1s = p_new_raw(struct thr_for_each_lvl1_job_t, count);

    if (count < thr_parallelism_g * 2) {
        struct thr_for_each_lvl0_job_t thr;

        thr.job.run = &thr_for_each_lvl0;
        thr.blk = blk;
        thr.from = 0;
        thr.to = count;
        thr.lvl1s = lvl1s;
        thr_for_each_lvl0(&thr.job, &syn);
    } else {
        size_t per_thread;

        lvl0s = p_new_raw(struct thr_for_each_lvl0_job_t, thr_parallelism_g);
        per_thread = DIV_ROUND_UP(count, thr_parallelism_g);

        for (size_t i = 0; i < thr_parallelism_g; i++) {
            struct thr_for_each_lvl0_job_t *t = &lvl0s[i];

            t->job.run = &thr_for_each_lvl0;
            t->blk = blk;
            t->from = i * per_thread;
            t->to = t->from + per_thread;
            t->lvl1s = &lvl1s[t->from];

            if (t->from >= count) {
                break;
            } else
            if (t->to > count) {
                t->to = count;
            }

            thr_syn_schedule(&syn, &t->job);
        }
    }

    thr_syn_wait(&syn);
    thr_syn_wipe(&syn);

    p_delete(&lvl1s);
    p_delete(&lvl0s);
}

/* }}} */
/* Tests {{{ */

#include <lib-common/z.h>

/* Wake up Thr0 {{{ */

struct {
    int count;
    uint64_t diff;
    el_t blocker;
} z_wake_up_g;

static int z_wake_up(void)
{
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);
    int iterations = 100000;

    if (fast) {
        iterations = 10000;
    }

    z_wake_up_g.blocker = el_blocker_register();
    thr_schedule_b(^{
        for (int i = 0; i < iterations; i++) {
            struct timeval start;

            assert (!thr_is_on_queue(thr_queue_main_g));
            lp_gettv(&start);
            thr_queue_sync_b(thr_queue_main_g, ^{
                struct timeval job;

                lp_gettv(&job);
                if (timeval_diff64(&job, &start) > 50000) {
                    z_wake_up_g.count++;
                }
                z_wake_up_g.diff += timeval_diff64(&job, &start);
            });
        }

        thr_queue_b(thr_queue_main_g, ^{
            el_unregister(&z_wake_up_g.blocker);
        });
    });
    while (z_wake_up_g.blocker) {
        el_loop_timeout(100);
    }
    Z_ASSERT_ZERO(z_wake_up_g.count);

    e_trace(3, "count: %d - %jd", z_wake_up_g.count,
            z_wake_up_g.diff / iterations);

    Z_HELPER_END;
}

/* }}} */
/* Post-Notify pattern {{{ */

struct {
    mpsc_queue_t queue;
    thr_job_t    job;
    thr_syn_t    syn;

    unsigned val __attribute__((aligned(64)));
    atomic_uint thr __attribute__((aligned(64)));
    unsigned mq  __attribute__((aligned(64)));
    atomic_uint pmq __attribute__((aligned(64)));
    atomic_bool flushed __attribute__((aligned(64)));
} z_post_notify_g;

struct post_notify_node {
    mpsc_node_t node;
    bool flush;
} __attribute__((aligned(64)));


static void run_post_check(mpsc_node_t *n, data_t data)
{
    assert (!container_of(n, struct post_notify_node, node)->flush);
    assert (!atomic_load(&z_post_notify_g.flushed));
}

static void run_post_notify(thr_job_t *job, thr_syn_t *syn)
{
    mpsc_it_t it;
    bool flush = false;

    mpsc_queue_drain_start(&it, &z_post_notify_g.queue);
    do {
        mpsc_node_t *n = mpsc_queue_drain_fast(&it, &run_post_check,
                                               (data_t){ .ptr = NULL });

        assert (!atomic_load(&z_post_notify_g.flushed));
        if (container_of(n, struct post_notify_node, node)->flush) {
            atomic_fetch_add(&z_post_notify_g.thr, 1);
            flush = true;
        }
    } while (!mpsc_queue_drain_end(&it, NULL));

    thr_syn_queue_b(&z_post_notify_g.syn, thr_queue_main_g, ^{
        if (flush) {
            z_post_notify_g.val++;
        }
        z_post_notify_g.mq++;
    });
    atomic_fetch_add(&z_post_notify_g.pmq, 1);
    if (flush) {
        bool exp = false;

        assert (atomic_compare_exchange_strong(&z_post_notify_g.flushed,
                                               &exp, true));
    }
}

/* }}} */
/* Contention {{{ */

struct contention_job {
    thr_job_t job;
    int       i;
};

static void run_contention(thr_job_t *job_, thr_syn_t *syn)
{
    struct contention_job *job = container_of(job_, struct contention_job, job);

    job->i *= 2;
}

/* }}} */
/* Sort {{{ */

static struct {
    int  help;

    int  sort_minsize;
} z_sort_g = {
    .sort_minsize = 128,
};

struct sort_job {
    thr_job_t  job;
    uint32_t  *a;
    size_t     n;
};

static void par_qsort_run(thr_job_t *job, thr_syn_t *syn);
static void par_qsort(thr_syn_t *syn, uint32_t *a, size_t n, bool use_blocks)
{
    uint32_t qstmp, qsop2;
    uint32_t *mean, *i, *j;
    size_t n1;

#define swap(p,q)        (qstmp = *(p), *(p) = *(q), *(q) = qstmp)
#define rotate(p,r,q)    (qstmp = *(p), *(p) = *(r), *(r) = *(q), *(q) = qstmp)
#define comp_gt(a,b)     (*(a) > *(b))

    for (;;) {
        i = a;
        j = a + n - 1;

        if (comp_gt(i, j)) {
            swap(i, j);
        }
        if (n <= 2)
            return;

        mean = i + (n >> 1);
        if (comp_gt(i, mean)) {
            swap(i, mean);
        } else
        if (comp_gt(mean, j)) {
            swap(j, mean);
        }
        if (n == 3)
            return;         /* in case of 3 items */

        qsop2 = *mean;       /* cause *mean is gonna change */

        for (;;) {
            /* We do not have guards in these loops because we assume
             * compare(x, x) returns 0.  If the comparison function is not
             * regular and returns != 0 on identical arguments, all bets
             * are off and this code may crash.
             */
            while (comp_gt(&qsop2, ++i)) {  /* find GE in left part */
                continue;
            }
            while (comp_gt(--j, &qsop2)) {  /* find LE in right part */
                continue;
            }

            if (i < j) {
                swap(i, j);        /* swap if needed */
                continue;
            }
            break;
        }

        /* i >= j : we're done and need to recurse on both ranges.
         * either i == j   : don't look at the ith element
         * or     i == j+1 : and split between them.
         */

        /* Fix ranges: [a..i[ U [j+1..a+n[ become [a..a+n[ U [j..j+n1[ */
        j++;
        n1 = a + n - j;
        n  = i - a;

        /* We want to recurse on [a..a+n[ and [j..j+n1[, but in order
         * to minimize stack usage, we only recurse on the smaller
         * range and loop on the other.
         */
        if (n <= n1) {
            SWAP(size_t, n, n1);
            SWAP(uint32_t *, a, j);
        }

        if (n1 > (size_t)z_sort_g.sort_minsize) {
            if (use_blocks) {
                thr_syn_schedule_b(syn, ^{
                    par_qsort(syn, j, n1, true);
                });
                thr_syn_schedule_b(syn, ^{
                    par_qsort(syn, a, n, true);
                });
            } else {
                struct sort_job *job;

                job = p_new_raw(struct sort_job, 1);
                job->job.run = &par_qsort_run;
                job->a    = j;
                job->n    = n1;
                thr_syn_schedule(syn, &job->job);

                job = p_new_raw(struct sort_job, 1);
                job->job.run = &par_qsort_run;
                job->a    = a;
                job->n    = n;
                thr_syn_schedule(syn, &job->job);
            }
            return;
        }

        if (n1 > 1) {
            par_qsort(syn, j, n1, use_blocks);
        }
    }
#undef intersects
#undef swap
#undef rotate
}

__flatten
static void par_qsort_run(thr_job_t *job, thr_syn_t *syn)
{
    struct sort_job *j = container_of(job, struct sort_job, job);

    par_qsort(syn, j->a, j->n, false);
    p_delete(&j);
}

static int test_qsort(bool use_blocks)
{
    t_scope;
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);
    const size_t len = fast ? (8 << 10) : (8 << 15);
    uint32_t *vec[thr_parallelism_g];
    thr_syn_t syn;

    for (size_t j = 0; j < thr_parallelism_g; j++) {
        vec[j] = p_new_raw(uint32_t, len);
        srand(0);
        for (size_t i = 0; i < len; i++) {
            vec[j][i] = ((unsigned)rand() << 16) | rand();
        }
    }

    z_sort_g.sort_minsize = MAX(2, z_sort_g.sort_minsize);
    thr_acc_reset();
    thr_syn_init(&syn);
    if (use_blocks) {
        for (size_t j = 0; j < thr_parallelism_g; j++) {
            uint32_t *a = vec[j];
            thr_syn_t *synp = &syn;

            thr_syn_schedule_b(&syn, ^{
                par_qsort(synp, a, len, true);
            });
        }
    } else {
        for (size_t j = 0; j < thr_parallelism_g; j++) {
            struct sort_job *job = p_new_raw(struct sort_job, 1);

            job->job.run    = &par_qsort_run;
            job->a          = vec[j];
            job->n          = len;
            thr_syn_schedule(&syn, &job->job);
        }
    }
    thr_syn_wait(&syn);

    thr_acc_trace(3, "%s", __func__);
    thr_syn_wipe(&syn);

    for (size_t j = 0; j < thr_parallelism_g; j++) {
        for (size_t i = 1; i < len; i++) {
            Z_ASSERT_LE(vec[j][i - 1], vec[j][i]);
        }
        p_delete(&vec[j]);
    }

    Z_HELPER_END;
}

/* }}} */
/* Queues {{{ */

struct test_queue {
    thr_queue_t *q;
    int i;
} __attribute__((aligned(64)));

static int test_queue(void)
{
    size_t n = thr_parallelism_g + 1;
    struct test_queue q[n];
    bool trace = e_is_traced(4);

    for (size_t i = 0; i < n; i++) {
        q[i].q = thr_queue_create();
        q[i].i = -1;
    }

    for (int i = 0; i < 20; i++) {
        for (size_t j = 0; j < n; j++) {
            struct test_queue *qj = &q[j];

            thr_queue_b(qj->q, ^{
                assert (qj->i == i - 1);
                qj->i = i;
                if (trace) {
                    fputc('1' + j, stderr);
                }
            });
        }
    }
    for (size_t j = 0; j < n; j++) {
        struct test_queue *qj = &q[j];

        thr_queue_b(qj->q, ^{
            assert (qj->i == 19);
            qj->i = 20;
            if (trace) {
                fputc('A' + j, stderr);
            }
        });
    }

    for (size_t i = 0; i < n; i++) {
        thr_queue_destroy(q[i].q, true);
        Z_ASSERT_EQ(q[i].i, 20);
    }

    if (trace) {
        fputc('\n', stderr);
    }
    Z_HELPER_END;
}

/* }}} */
/* Queue Sync {{{ */

static size_t queue_sync_g;

static int test_queue_sync(void)
{
    t_scope;
    thr_syn_t *syn = t_new_raw(thr_syn_t, 1);
    thr_queue_t *q = thr_queue_create();

    queue_sync_g = 0;
    thr_syn_init(syn);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        thr_syn_schedule_b(syn, ^{
            for (int j = 0; j < 10; j++) {
                thr_syn_schedule_b(syn, ^{
                    for (int k = 0; k < 100; k++) {
                        thr_queue_sync_b(q, ^{
                            queue_sync_g++;
                        });
                    }
                });
            }
        });
    }
    thr_syn_wait(syn);
    thr_syn_wipe(syn);
    thr_queue_destroy(q, true);

    Z_ASSERT_EQ(queue_sync_g, 10 * thr_parallelism_g * 100);
    Z_HELPER_END;
}

/* }}} */
/* Steal job {{{ */

typedef struct rci_t {
    mpsc_node_t node;
    int   seqid;
    byte *data;
} rci_t;
qvector_t(rci, rci_t *);

static struct {
    qv_t(rci) ci_ring;
    mpsc_queue_t mpsc;
    int last_acked;
} queue_steal_g = {
    .last_acked = -1,
};

static void run_node(mpsc_node_t *node, data_t d)
{
    int *unqueued = d.ptr;

    usleep(1);
    (*unqueued)++;
}

static void run_job(thr_job_t *j, thr_syn_t *syn)
{
    mpsc_it_t it;
    int to_sync;
    int unqueued = 0;

    mpsc_queue_drain_start(&it, &queue_steal_g.mpsc);
    do {
        mpsc_node_t *node;
        rci_t *rci;

        node = mpsc_queue_drain_fast(&it, run_node,
                                     (data_t){ .ptr = &unqueued });

        rci = container_of(node, rci_t, node);
        to_sync = rci->seqid;
        usleep(5);
        unqueued++;
    } while (!mpsc_queue_drain_end(&it, NULL));

    thr_syn_queue_b(syn, thr_queue_main_g, ^{
        while (queue_steal_g.last_acked < to_sync) {
            rci_t *rci = queue_steal_g.ci_ring.tab[0];

            queue_steal_g.last_acked++;
            assert (queue_steal_g.last_acked == rci->seqid);
            p_delete(&rci->data);
            p_delete(&rci);
            qv_skip(&queue_steal_g.ci_ring, 1);
        }
    });
}

/* }}} */
/* {{{ Test thr_for_each */

struct thr_int_td_t {
    thr_td_t td;
    uint64_t sum;
};

static int z_thr_for_each(void)
{
    thr_syn_t *syn = thr_syn_new();
    __block uint64_t res = 0;
    __block uint64_t tds = 0;
    uint64_t sum;

    thr_syn_declare_td(syn, ^{
        return &p_new(struct thr_int_td_t, 1)->td;
    }, ^(thr_td_t **ptd) {
        p_delete(ptd);
    });

    thr_for_each(1000000, ^(size_t pos) {
        struct thr_int_td_t *td;

        td = container_of(thr_syn_acquire_td(syn), struct thr_int_td_t, td);
        td->sum += pos;
        thr_syn_release_td(syn, &td->td);
    });

    thr_syn_collect_td(syn, ^(const thr_td_t *ttd) {
        const struct thr_int_td_t *td;

        td = container_of(ttd, const struct thr_int_td_t, td);
        res += td->sum;
        tds++;
    });

    thr_syn_delete(&syn);

    sum = res;
    Z_ASSERT_EQ(sum, 499999500000ull);

    sum = tds;
    Z_ASSERT_LE(sum, thr_parallelism_g);

    Z_HELPER_END;
}

Z_GROUP_EXPORT(thrjobs) {
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);

    MODULE_REQUIRE(thr);

    Z_TEST(contention, "test contention behavior") {
        struct contention_job jobs[2 * thr_parallelism_g * THR_JOB_MAX];
        int iterations = 1 << 10;

        if (fast) {
            iterations = 1 << 5;
        }

        for (int j = 0; j < iterations; j++) {
            thr_syn_t syn;

            thr_acc_reset();
            thr_syn_init(&syn);
            for (int i = 0; i < countof(jobs); i++) {
                jobs[i] = (struct contention_job) {
                    .job.run = run_contention,
                    .i       = i,
                };
                thr_syn_schedule(&syn, &jobs[i].job);
            }

            thr_syn_wait(&syn);

            for (int i = 0; i < countof(jobs); i++) {
                Z_ASSERT_EQ(jobs[i].i, i * 2);
            }

            thr_acc_trace(3, "");
            thr_syn_wipe(&syn);
        }
    } Z_TEST_END;

    Z_TEST(sort_job, "test sort with thr_job_t structure") {
        for (int i = 0; i < 16; i++) {
            z_sort_g.sort_minsize = 1 << i;
            Z_HELPER_RUN(test_qsort(false));
        }
    } Z_TEST_END;

    Z_TEST(sort_block, "test sort with blocks") {
        for (int i = 0; i < 16; i++) {
            z_sort_g.sort_minsize = 1 << i;
            Z_HELPER_RUN(test_qsort(true));
        }
    } Z_TEST_END;

    Z_TEST(queue, "test queues") {
        Z_HELPER_RUN(test_queue());
    } Z_TEST_END;

    Z_TEST(queue_syn, "tests queue sync") {
        int loops = fast ? 100 : 10000;

        for (int i = 0; i < loops; i++) {
            Z_HELPER_RUN(test_queue_sync());
        }
    } Z_TEST_END;

    Z_TEST(wake_up_thr0, "test the main thread wake up procedure") {
        Z_HELPER_RUN(z_wake_up());
    } Z_TEST_END;

    Z_TEST(post_notify, "test the post-notify pattern") {
        int iterations = 300000; /* approximatively 1s */

        mpsc_queue_init(&z_post_notify_g.queue);
        thr_syn_init(&z_post_notify_g.syn);
        z_post_notify_g.job.run = &run_post_notify;

        if (fast) {
            iterations = 50000;
        }

        for (int loops = 0; loops < iterations; loops++) {
            t_scope;
            int count = rand() % 32;
            bool posted_flush = false;
            int posted = 0;
            unsigned val = z_post_notify_g.val;
            unsigned mq  = z_post_notify_g.mq;
            struct post_notify_node *nodes = t_new(struct post_notify_node,
                                                   count + 1);

            atomic_store(&z_post_notify_g.flushed, false);
            for (int i = 0; i < count; i++) {
                nodes[i].flush = false;
                if (mpsc_queue_push(&z_post_notify_g.queue, &nodes[i].node)) {
                    thr_syn_schedule(&z_post_notify_g.syn, &z_post_notify_g.job);
                    posted++;
                }
            }
            nodes[count].flush = true;
            if (mpsc_queue_push(&z_post_notify_g.queue, &nodes[count].node)) {
                thr_syn_schedule(&z_post_notify_g.syn, &z_post_notify_g.job);
                posted_flush = true;
                posted++;
            }
            thr_syn_wait(&z_post_notify_g.syn);

            e_trace(3, "val=%d thr=%d => jobs=%d mq=%d pmq=%d flush=%d",
                    z_post_notify_g.val, atomic_load(&z_post_notify_g.thr),
                    mq + posted, z_post_notify_g.mq,
                    atomic_load(&z_post_notify_g.pmq), posted_flush);
            Z_ASSERT_EQ(val + 1, z_post_notify_g.val);
            Z_ASSERT_EQ(z_post_notify_g.val, atomic_load(&z_post_notify_g.thr));
            Z_ASSERT_EQ(atomic_load(&z_post_notify_g.pmq), mq + posted);
            Z_ASSERT_EQ(z_post_notify_g.mq, mq + posted);
            Z_ASSERT(atomic_load(&z_post_notify_g.flushed));
        }

        thr_syn_wipe(&z_post_notify_g.syn);
    } Z_TEST_END;

    Z_TEST(queue_steal, "queue steal jobs") {
        int seqid = 0;
        thr_job_t job = { .run = &run_job };
        struct timeval start;

        mpsc_queue_init(&queue_steal_g.mpsc);

        while (seqid < 30000) {
            rci_t *rci = p_new(rci_t, 1);

            rci->data  = p_new(byte, 16 << 10);
            rci->seqid = seqid++;

            qv_append(&queue_steal_g.ci_ring, rci);

            if (mpsc_queue_push(&queue_steal_g.mpsc, &rci->node)) {
                thr_schedule(&job);
            }

            el_loop_timeout(queue_steal_g.ci_ring.len > 1000 ? 0 : 10);
        }

        lp_gettv(&start);
        for (;;) {
            struct timeval end;
            int64_t diff;

            lp_gettv(&end);
            diff = timeval_diffmsec(&end, &start);
            if (diff > 1000) {
                break;
            }
            el_loop_timeout(1000 - diff);
            if (queue_steal_g.last_acked == seqid - 1) {
                break;
            }
        }
        Z_ASSERT_EQ(queue_steal_g.last_acked, seqid - 1);
        Z_ASSERT_ZERO(queue_steal_g.ci_ring.len);
    } Z_TEST_END;

    Z_TEST(for_each, "thr for each") {
        Z_HELPER_RUN(z_thr_for_each());
    } Z_TEST_END;

    MODULE_RELEASE(thr);
} Z_GROUP_END;

/* }}} */
