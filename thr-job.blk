/**************************************************************************/
/*                                                                        */
/*  Copyright (C) INTERSEC SA                                             */
/*                                                                        */
/*  Should you receive a copy of this source code, you must check you     */
/*  have a proper, written authorization of INTERSEC to hold it. If you   */
/*  don't have such an authorization, you must DELETE all source code     */
/*  files in your possession, and inform INTERSEC of the fact you obtain  */
/*  these files. Should you not comply to these terms, you can be         */
/*  prosecuted in the extent permitted by applicable law.                 */
/*                                                                        */
/**************************************************************************/

#include <sys/syscall.h>
#include <lib-common/arith.h>
#include <lib-common/datetime.h>
#include <lib-common/container.h>
#include <lib-common/el.h>
#include <lib-common/unix.h>
#include "thr.h"

#if !defined(__x86_64__) && !defined(__i386__)
#  error "this file assumes a strict memory model and is probably buggy on !x86"
#endif

#ifndef SYS_eventfd
#  error "this file assumes a glibc that knows about the eventfd syscall"
#endif

static int eventfd(int initialvalue, int flags)
{
    int fd = RETHROW(syscall(SYS_eventfd, initialvalue));

    fd_set_features(fd, flags);
    return fd;
}

#define CACHE_LINE_SIZE   64

typedef struct thr_qnode_t thr_qnode_t;
struct thr_qnode_t {
    mpsc_node_t  qnode;
    thr_job_t   *job;
    thr_syn_t   *syn;
} __attribute__((aligned(CACHE_LINE_SIZE)));

struct thr_queue_t {
    thr_job_t    run;
    thr_job_t    destroy;
    mpsc_queue_t q;
} __attribute__((aligned(CACHE_LINE_SIZE)));

typedef struct thr_info_t {
    /** top of the deque.
     * this variable is accessed through shared_read, and modified through
     * atomic_bool_cas which ensure load/store consistency. Hence no barrier
     * is needed to read it.
     */
    unsigned   top;
    /** bottom of the deque.
     * this variable is only modified by the current thread, hence it doesn't
     * need read barriers to read it. Though other threads do need a read
     * barrier before acces, and the owner of the deque must publish new
     * values through a write barrier.
     */
    unsigned   bot;
    unsigned   alive;
    pthread_t  thr;
    char       padding_0[CACHE_LINE_SIZE];

    struct deque_entry {
        thr_job_t *job;
        thr_syn_t *syn;
    } q[THR_JOB_MAX];
    char       padding_1[CACHE_LINE_SIZE];

#define NCACHE_MAX    1024
    size_t       ncache_sz;
    thr_qnode_t  ncache;

#ifndef NDEBUG
    struct thr_acc {
        uint64_t time;
        uint64_t ec_wait_time;
        uint64_t ec_steal_time;

        unsigned jobs_queued;
        unsigned jobs_run;
        unsigned ec_gets;
        unsigned ec_waits;
        unsigned jobs_steals;
        unsigned jobs_stealed;
        unsigned jobs_failed_steals;
        unsigned jobs_failed_dequeues;
    } acc;
#endif
} thr_info_t;

static struct {
    unsigned volatile stopping;
    thr_evc_t         ec;
    pthread_barrier_t start_bar;

    thr_info_t       *threads;
    el_t              before;
    el_t              eventel;
    int               efd;
    bool              use_eventfd;
    thr_queue_t       main_queue;
    uint64_t          reset_time;
    proctimer_t       st;
} thr_job_g = {
#define _G  thr_job_g
    .efd = -1
};

static __thread int self_tid_g = (-1);
static __thread thr_info_t *self_g;

size_t thr_parallelism_g;
thr_queue_t *const thr_queue_main_g = &_G.main_queue;
static thr_evc_t *thr0_cur_ec_g;

/* Tracing {{{ */
#ifndef NDEBUG

void thr_acc_reset(void)
{
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        p_clear(&_G.threads[i].acc, 1);
    }
    _G.reset_time = hardclock();
    proctimer_start(&_G.st);
}

static size_t int_width(uint64_t i)
{
    int w = 1;

    for (; i > 1; i /= 10)
        w++;
    return w;
}

void thr_acc_set_affinity(size_t offs)
{
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        cpu_set_t set;

        CPU_ZERO(&set);
        CPU_SET((i + offs) % thr_parallelism_g, &set);
        if (pthread_setaffinity_np(_G.threads[i].thr, sizeof(set), &set))
            e_panic("cannot set affinity");
    }
}

void thr_acc_trace(int lvl, const char *fmt, ...)
{
    uint64_t avg, wall = hardclock() - _G.reset_time;
    unsigned waste, speedup;

    struct thr_acc total = { .time = 0, };
    struct thr_acc width = { .time = 0, };
    va_list  ap;
    SB_8k(sb);

    proctimer_stop(&_G.st);

    va_start(ap, fmt);
    sb_addvf(&sb, fmt, ap);
    va_end(ap);

    width.time = int_width(wall / 1000000);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        struct thr_acc *acc = &_G.threads[i].acc;

        total.time        += acc->time;
        total.jobs_queued += acc->jobs_queued;
        total.jobs_run    += acc->jobs_run;
        total.ec_gets     += acc->ec_gets;
        total.ec_waits    += acc->ec_waits;
        total.ec_wait_time += acc->ec_wait_time;
        total.jobs_steals   += acc->jobs_steals;
        total.jobs_stealed  += acc->jobs_stealed;
        total.jobs_failed_steals += acc->jobs_failed_steals;
        total.jobs_failed_dequeues += acc->jobs_failed_dequeues;

        width.time         = MAX(width.time,        int_width(acc->time / 1000000));
        width.jobs_queued  = MAX(width.jobs_queued, int_width(acc->jobs_queued));
        width.jobs_run     = MAX(width.jobs_run,    int_width(acc->jobs_run));
        width.ec_gets      = MAX(width.ec_gets,     int_width(acc->ec_gets));
        width.ec_waits     = MAX(width.ec_waits,    int_width(acc->ec_waits));
        width.ec_wait_time = MAX(width.ec_wait_time, int_width(acc->ec_wait_time / 1000000));
        width.jobs_steals    = MAX(width.jobs_steals,   int_width(acc->jobs_steals));
        width.jobs_stealed   = MAX(width.jobs_stealed,   int_width(acc->jobs_stealed));
        width.jobs_failed_steals = MAX(width.jobs_failed_steals,
                                       int_width(acc->jobs_failed_steals));
        width.jobs_failed_dequeues = MAX(width.jobs_failed_dequeues,
                                       int_width(acc->jobs_failed_dequeues));
    }

    e_trace(lvl, "----- %*pM", sb.len, sb.data);

    if (total.jobs_run == 0) {
        e_trace(lvl, "----- No jobs since last reset");
        return;
    }

#define TIME_FMT_ARG(t)   (int)width.time, (int)((t) / 1000000)

    sb_reset(&sb);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        struct thr_acc *acc = &_G.threads[i].acc;

        e_trace(lvl, " %2zd: %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
                "%*u failed dequeues, %*u gets, %*u waits (%*uM)", i,
                TIME_FMT_ARG(acc->time),
                width.jobs_queued, acc->jobs_queued,
                width.jobs_run,    acc->jobs_run,
                width.jobs_steals, acc->jobs_steals,
                width.jobs_stealed, acc->jobs_stealed,
                width.jobs_failed_steals, acc->jobs_failed_steals,
                width.jobs_failed_dequeues, acc->jobs_failed_dequeues,
                width.ec_gets,     acc->ec_gets,
                width.ec_waits,    acc->ec_waits,
                TIME_FMT_ARG(acc->ec_wait_time));
    }
    e_trace(lvl, "wall %*uM, %*u queued, %*u run, %*u steals (%*u jobs, %*u failed), "
            "%*u failed dequeues, %*u gets, %*u waits (%*uM)", TIME_FMT_ARG(wall),
            width.jobs_queued, total.jobs_queued,
            width.jobs_run,    total.jobs_run,
            width.jobs_steals, total.jobs_steals,
            width.jobs_stealed, total.jobs_stealed,
            width.jobs_failed_steals, total.jobs_failed_steals,
            width.jobs_failed_dequeues, total.jobs_failed_dequeues,
            width.ec_gets,     total.ec_gets,
            width.ec_waits,    total.ec_waits,
            TIME_FMT_ARG(total.ec_wait_time));
    avg     = (uint64_t)(total.time / thr_parallelism_g);
    waste   = (uint64_t)(wall - avg) * 10000 / wall;
    speedup = total.time * 100 / wall;
    e_trace(lvl, "avg  %*uM (%d.%02d%% waste, %d.%02dx speedup)",
            TIME_FMT_ARG(avg), waste / 100, waste % 100,
            speedup / 100, speedup % 100);
    e_trace(lvl, "job  %*jd cycles in avg",
            (int)width.time, total.time / total.jobs_run);
    e_trace(lvl, "cost %*jd cycles of overhead per job", (int)width.time,
            (wall * thr_parallelism_g - total.time) / total.jobs_run);
    e_trace(lvl, "     %s", proctimer_report(&_G.st, NULL));
#undef TIME_FMT_ARG
}

#endif
/* }}} */
/* atomic dequeue {{{ */

static bool job_run(thr_job_t *job, thr_syn_t *syn)
{
#ifndef NDEBUG
    unsigned long start = hardclock();

    self_g->acc.jobs_run++;
#endif

    if ((uintptr_t)job & 3) {
        block_t blk = (block_t)((uintptr_t)job & ~(uintptr_t)3);

        blk();
        if ((uintptr_t)job & 1)
            Block_release(blk);
    } else {
        (*job->run)(job, syn);
    }
#ifndef NDEBUG
    self_g->acc.time += (hardclock() - start);
#endif
    if (syn)
        thr_syn__job_done(syn);
    return true;
}

void thr_syn_schedule(thr_syn_t *syn, thr_job_t *job)
{
    unsigned bot, top;
    struct deque_entry *e;

#ifndef NDEBUG
    self_g->acc.jobs_queued++;
#endif

    if (syn)
        thr_syn__job_prepare(syn);

    /* Read the current limits of the queue. Since 'bot' cannot be changed by
     * another thread and 'top' can only be moved in the direction of
     * consuming threads, 'bot' is effectively the position where the job
     * should be inserted and 'bot - top' gives an upper bound to the number of
     * job in the queue (and is the actual number if no other thread try to
     * steal a job to that one concurrently to the insertion).
     */
    bot = shared_read(self_g->bot);
    top = shared_read(self_g->top);

    /* Looks like there may be too many jobs in the queue of that thread, run
     * the new one immediately.
     */
    if (unlikely((int)(bot - top) >= THR_JOB_MAX)) {
        job_run(job, syn);
        return;
    }

    /* Add the job in the queue and update bottom. Since other threads can
     * only consume jobs from the queue (increment top), we can safely add the
     * new job at q[bot] and then increment bot
     */
    e = &self_g->q[bot % THR_JOB_MAX];
    shared_write(e->job, job);
    shared_write(e->syn, syn);
    shared_write(self_g->bot, bot + 1);

    thr_ec_signal(&_G.ec);
}

void thr_schedule(thr_job_t *job)
{
    thr_syn_schedule(NULL, job);
}


/** Consume the top job of the specified thread.
 *
 * @param ti    the thread info structure of the thread to update
 * @param top__ expected value of the 'top' line.
 * @return true in case of success, false if another thread already fetched
 *         the top element.
 */
#define cas_top(ti, top__, count__) \
    likely(atomic_bool_cas(&(ti)->top, top__, top__ + count__))


/** Run the 'bottom' job of the queue of the current thread.
 *
 * @return true if a job has been run, false if the queue is empty.
 */
static bool thr_job_dequeue(void)
{
    struct deque_entry *e;
    unsigned top, bot;

    /* Read the bottom job and update the mark to mark that job as consumed.
     * The remaining of the function will ensure we were effectively the first
     * to reclaim the ownership of that job.
     */
    bot = shared_read(self_g->bot) - 1;
    shared_write(self_g->bot, bot);
    mb();

    /* Read the top line. The memory barrier ensure that the read is effectively
     * done after updating bot and thus that other thread may have seen the
     * update of bot before touching top.
     */
    top = shared_read(self_g->top);

    /* There are remaining jobs after moving the bottom line. Top has been
     * read after bot, and since bot is refetched before each individual
     * trysteal,  we're sure 'top' cannot have reached 'bot', and thus
     * we are the owner of the job we fetched, run it.
     */
    if (likely((int)(bot - top) > 0)) {
        e = &self_g->q[bot % THR_JOB_MAX];
        return job_run(e->job, e->syn);
    }

    /* 'bot' and 'top' are equal, that mean that either we're consuming the
     * last job of the queue or another thread did it. To know if the job
     * is for us, check that top hasn't been moved. If it has been moved, the
     * CAS will fail and the job is not for us, if it hasn't been moved, then
     * move it to ensure nobody else will do it, then, move back 'bot' and
     * 'top' to the same point since the queue is empty.
     */
    if (likely(bot == top)) {
        if (cas_top(self_g, top, 1)) {
            shared_write(self_g->bot, top + 1);
            e = &self_g->q[bot % THR_JOB_MAX];
            return job_run(e->job, e->syn);
        } else {
#ifndef NDEBUG
            self_g->acc.jobs_failed_dequeues++;
#endif
        }
    }

    /* 'top' has been moved to the previous value of bot, this means the job
     * has already been run by another thread. Since top > bot, we're
     * sure nobody else will try to read another job and that
     * top == bot + 1 == previous value of bot. Thus, we have to restore 'bot'
     * to its previous value since we didn't get ownership of the job.
     */
    shared_write(self_g->bot, bot + 1);
    return false;
}

/** Try to steal a job from the queue of another thread.
 *
 * @param ti  The thread info structure of another thread.
 * @return 1 if a job has been stolen, 0 if the thread's queue is empty,
 *         -1 if the attempt failed (a retry may be needed)
 */
static int thr_job_try_steal(thr_info_t *ti, int depth)
{
    unsigned top, bot;

    /* Read the limits of the queue of the thread and fetch the top 'job' of
     * the queue.
     */
    top = shared_read(ti->top);
    bot = shared_read(ti->bot);

    /* If the queue does not seem to be empty, then we know we own the job if
     * and only if we can CAS top. This works because a concurrent
     * dequeue/steal will always CAS top when consuming the last job of the
     * queue. The tricky part is the concurrency between dequeue and steal
     * solved by the fact we'll always move 'bot' and 'top' in dequeue when
     * emptying the queue.
     */
    if ((int)(bot - top) > 0) {
        unsigned to_read = 1; // DIV_ROUND_UP(bot - top, (1 << (depth + 1)));
        unsigned my_bot = shared_read(self_g->bot);
        struct deque_entry e;

        rmc();
        e = access_once(ti->q[top % THR_JOB_MAX]);
        for (unsigned i = 1; i < to_read; i++) {
            access_once(self_g->q[(my_bot + i - 1) % THR_JOB_MAX])
                = access_once(ti->q[(top + i) % THR_JOB_MAX]);
        }
        wmc();

        if (cas_top(ti, top, to_read)) {
            if (to_read > 1) {
                shared_write(self_g->bot, my_bot + to_read - 1);
            }
#ifndef NDEBUG
            self_g->acc.jobs_stealed += to_read;
            self_g->acc.jobs_steals++;
#endif
            return job_run(e.job, e.syn);
        } else {
#ifndef NDEBUG
            self_g->acc.jobs_failed_steals++;
#endif
            return -1;
        }
    }
    return 0;
}

#undef cas_top

/* FIXME: optimize for large number of threads, with a loopless fastpath */
static int thr_job_steal(void)
{
    bool empty = true;

    for (size_t i = 1; i < thr_parallelism_g; i++) {
        size_t tid = (i + self_tid_g) % thr_parallelism_g;
        int res;

        res = thr_job_try_steal(&_G.threads[tid], i);
        if (res > 0) {
            return 1;
        } else
        if (res < 0) {
            empty = false;
        }
        sched_yield();
    }
    return empty ? 0 : -1;
}

/* }}} */
/* serial queues {{{ */

static ALWAYS_INLINE thr_qnode_t *thr_qnode_of(mpsc_node_t *node)
{
    return container_of(node, thr_qnode_t, qnode);
}

static thr_qnode_t *thr_qnode_create(void)
{
    if (likely(self_g && self_g->ncache.qnode.next)) {
        thr_qnode_t *n = thr_qnode_of(self_g->ncache.qnode.next);

        self_g->ncache.qnode.next = n->qnode.next;
        self_g->ncache_sz--;
        return n;
    }
    return memalign(64, sizeof(thr_qnode_t));
}

static void thr_qnode_destroy(mpsc_node_t *n)
{
    if (likely(self_g && self_g->ncache_sz < NCACHE_MAX)) {
        self_g->ncache_sz++;
        n->next = self_g->ncache.qnode.next;
        self_g->ncache.qnode.next = n;
    } else {
        free(thr_qnode_of(n));
    }
}

static void thr_queue_wipe(thr_queue_t *q)
{
    assert (mpsc_queue_looks_empty(&q->q));
}
GENERIC_DELETE(thr_queue_t, thr_queue);

static void thr_queue_drain(thr_queue_t *q)
{
    bool wipe = false;
    mpsc_it_t it;

    if (unlikely(mpsc_queue_looks_empty(&q->q))) {
        /* The queue should not be marked empty *and* scheduled. the only
         * queue that should go through this slowpath is the main queue
         * since it's not really scheduled like the other ones
         */
        assert (q == thr_queue_main_g);
        return;
    }

#define doit(n) \
    ({  thr_job_t *job = thr_qnode_of(n)->job; \
        thr_syn_t *syn = thr_qnode_of(n)->syn; \
                                               \
        thr_qnode_destroy(n);                  \
        job_run(job, syn);                     \
    })
    mpsc_queue_drain_start(&it, &q->q);
    do {
        thr_qnode_t *n = thr_qnode_of(mpsc_queue_drain_fast(&it, doit));
        job_run(n->job, n->syn);
        wipe = (n->job == &q->destroy);
    } while (!__mpsc_queue_drain_end(&it, thr_qnode_destroy, cpu_relax()));
#undef doit

    if (wipe) {
        thr_queue_delete(&q);
    }
}

static void thr_queue_run(thr_job_t *job, thr_syn_t *syn)
{
    thr_queue_drain(container_of(job, thr_queue_t, run));
}

static void thr_queue_finalize(thr_job_t *job, thr_syn_t *syn)
{
    /*
     * Do nothing in the finalize, thr_queue_drain will perform the deletion
     * itself. This function serves as a marker that we reached the end of the
     * queue.
     */
}

static thr_queue_t *thr_queue_init(thr_queue_t *q)
{
    p_clear(q, 1);
    mpsc_queue_init(&q->q);
    q->run.run     = &thr_queue_run;
    q->destroy.run = &thr_queue_finalize;
    return q;
}

static void thr_wakeup_thr0(void)
{
    if (self_tid_g != 0) {
        thr_evc_t *ec = atomic_xchg(&thr0_cur_ec_g, NULL);

        if (ec) {
            thr_ec_broadcast(ec);
            shared_write(thr0_cur_ec_g, ec);
        } else {
            uint64_t val = 1;

            IGNORE(write(_G.efd, &val, sizeof(val)));
        }
    }
}

void thr_syn_queue(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    if (likely(q)) {
        thr_qnode_t *n = thr_qnode_create();

        n->job = job;
        n->syn = syn;
        if (syn)
            thr_syn__job_prepare(syn);

        if (mpsc_queue_push(&q->q, &n->qnode)) {
            if (q == thr_queue_main_g) {
                thr_wakeup_thr0();
            } else {
                thr_schedule(&q->run);
            }
        }
    } else {
        thr_syn_schedule(syn, job);
    }
}

void thr_queue_sync(thr_queue_t *q, thr_job_t *job)
{
    thr_qnode_t *n;
    thr_syn_t syn;

    thr_syn_init(&syn);
    thr_syn__job_prepare(&syn);
    n = thr_qnode_create();
    n->job = job;
    n->syn = &syn;
    /*
     * if mpsc_queue_push returns 1, then we're alone !
     */
    if (mpsc_queue_push(&q->q, &n->qnode)) {
        if (q == thr_queue_main_g && self_tid_g != 0) {
            thr_wakeup_thr0();
        } else {
            thr_queue_drain(q);
            return;
        }
    }
    thr_syn_wait(&syn);
    thr_syn_wipe(&syn);
}

void thr_queue(thr_queue_t *q, thr_job_t *job)
{
    thr_syn_queue(NULL, q, job);
}

thr_queue_t *thr_queue_create(void)
{
    thr_queue_t *q = p_new(thr_queue_t, 1);

    return thr_queue_init(q);
}

void thr_queue_destroy(thr_queue_t *q, bool wait)
{
    assert (q != thr_queue_main_g);
    if (wait) {
        thr_queue_sync(q, &q->destroy);
    } else {
        thr_queue(q, &q->destroy);
    }
}

/* }}} */
/* thread run loop {{{ */

static void thr_info_init(int tid)
{
    self_tid_g = tid;
    self_g     = _G.threads + tid;
    shared_write(self_g->alive, true);
    shared_write(self_g->thr, pthread_self());
    pthread_barrier_wait(&_G.start_bar);
    mb();
}

static void thr_info_cleanup(void *unused)
{
    do {
        if (self_tid_g == 0)
            thr_queue_drain(thr_queue_main_g);
    } while (thr_job_dequeue());
    if (self_tid_g == 0)
        thr_queue_wipe(thr_queue_main_g);
    while (self_g->ncache.qnode.next) {
        thr_qnode_t *n = thr_qnode_of(self_g->ncache.qnode.next);

        self_g->ncache.qnode.next = n->qnode.next;
        free(n);
    }
    mb();
    shared_write(self_g->alive, false);
}

static void *thr_job_main(void *arg)
{
    thr_info_init((uintptr_t)arg);
    pthread_cleanup_push(thr_info_cleanup, NULL);

    while (likely(!shared_read(_G.stopping))) {
        int res;
        uint64_t key = 0;

        while (likely(thr_job_dequeue())) {
            continue;
        }
        if (thr_job_steal() > 0) {
            continue;
        }

        do {
            sched_yield();
            key = thr_ec_get(&_G.ec);
#ifndef NDEBUG
            self_g->acc.ec_gets++;
#endif
        } while ((res = thr_job_steal()) < 0);
        if (res == 0 && !shared_read(_G.stopping)) {
#ifndef NDEBUG
            unsigned long start = hardclock();

            self_g->acc.ec_waits++;
#endif
            thr_ec_wait(&_G.ec, key);

#ifndef NDEBUG
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
        }
    }
    pthread_cleanup_pop(1);
    return NULL;
}

/* }}} */
/* Module init / shutdown {{{ */

static void thr_job_atfork(void)
{
    el_before_unregister(&_G.before);
    el_fd_unregister(&_G.eventel, true);
    if (!_G.use_eventfd) {
        p_close(&_G.efd);
    }
    p_delete(&_G.threads);
    p_clear(&_G, 1);
    thr_parallelism_g = 0;
}

static void thr_before(el_t ev, el_data_t priv)
{
    thr_queue_drain(thr_queue_main_g);
}

static int thr_event(el_t ev, int fd, short evt, el_data_t priv)
{
    uint64_t val[16];

    if (_G.use_eventfd) {
        RETHROW(read(fd, val, sizeof(uint64_t)));
    } else {
        while (RETHROW(read(fd, val, sizeof(val))) == sizeof(val));
    }

    thr_queue_drain(thr_queue_main_g);
    return 0;
}

void thr_queue_main_drain(void)
{
    assert (self_tid_g == 0);
    thr_queue_drain(thr_queue_main_g);
    thr_ec_signal(&_G.ec);
}

void thr_initialize(void)
{
#ifndef SHARED
    static bool atfork_registered;
#endif
    sigset_t fillset;
    sigset_t old;
    pthread_t tid;
    pthread_attr_t attr;

    if (!thr_parallelism_g) {
        const char *env = getenv("THR_MAX_PARALLELISM");
        size_t nb_cpu = sysconf(_SC_NPROCESSORS_CONF);

        if (env && *env) {
            thr_parallelism_g = atoi(env);
            if (thr_parallelism_g < 2) {
                e_fatal("invalid THR_PARALLELISM value `%s`", env);
            }
            thr_parallelism_g = MIN(thr_parallelism_g, nb_cpu);
        } else {
            thr_parallelism_g = nb_cpu;
        }
        thr_parallelism_g = MAX(thr_parallelism_g, 2);

        if (unlikely(mem_tool_is_running(MEM_TOOL_VALGRIND)))
            thr_parallelism_g = MIN(2, thr_parallelism_g);
        _G.threads = p_new(thr_info_t, thr_parallelism_g);

        pthread_attr_init(&attr);
        pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
        sigfillset(&fillset);
        pthread_sigmask(SIG_SETMASK, &fillset, &old);
        pthread_barrier_init(&_G.start_bar, NULL, thr_parallelism_g);
        for (size_t i = 1; i < thr_parallelism_g; i++) {
            if (thr_create(&tid, &attr, thr_job_main, (void *)(uintptr_t)i)) {
                e_fatal("unable to create new thread: %m");
            }
        }
        thr_info_init(0);
        pthread_barrier_destroy(&_G.start_bar);
        pthread_sigmask(SIG_SETMASK, &old, NULL);
        pthread_attr_destroy(&attr);
        thr_queue_init(thr_queue_main_g);
        _G.before = el_unref(el_before_register(thr_before, NULL));

        /* Use eventfd if available, since it's much more efficient than
         * the pipe alternative. (Benches show that eventfd wake up the main
         * thread in 7 micro-seconds while a pipe wake up the main thread in
         * 11 micro-seconds)
         */
        _G.efd = eventfd(0, O_NONBLOCK | O_CLOEXEC);
        if (_G.efd >= 0) {
            _G.eventel = el_fd_register(_G.efd, POLLIN, &thr_event, NULL);
            _G.use_eventfd = true;
        } else {
            int fd[2] = { -1, -1 };

            if (pipe(fd) < 0) {
                e_panic("cannot create pipe: %m");
            }

            fd_set_features(fd[0], O_NONBLOCK | O_CLOEXEC);
            fd_set_features(fd[1], O_NONBLOCK | O_CLOEXEC);
            _G.eventel = el_fd_register(fd[0], POLLIN, &thr_event, NULL);
            _G.efd = fd[1];
        }
        el_unref(_G.eventel);
    }

#ifndef SHARED
    /* XXX done at the end of the function to ensure the el's at_fork is
     * registered before the thr_job's at fork. Since pthread_atfork()
     * guarantees the order of execution, this ensure thr_job_atfork won't
     * affect the parent process' event loop.
     */
    if (!atfork_registered) {
        pthread_atfork(NULL, NULL, thr_job_atfork);
        atfork_registered = true;
    }
#endif
}

__attribute__((destructor))
void thr_shutdown(void)
{
    if (thr_parallelism_g && self_tid_g == 0) {
        shared_write(_G.stopping, true);
        thr_ec_broadcast(&_G.ec);

        for (size_t i = 1; i < thr_parallelism_g; i++) {
            mb();
            while (access_once(_G.threads[i].alive)) {
                thr_ec_broadcast(&_G.ec);
                sched_yield();
            }
        }

        thr_info_cleanup(NULL);

        thr_job_atfork();
    }
}

/* }}} */
/* thr_syn {{{ */

size_t thr_id(void)
{
    return self_tid_g;
}

void thr_syn_wait(thr_syn_t *syn)
{
#ifndef NDEBUG
    unsigned long start;
#endif

    thr_syn__retain(syn);

    if (self_tid_g == 0) {
        while (shared_read(syn->pending)) {
            uint64_t key;

            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                thr_queue_drain(thr_queue_main_g);
                continue;
            }
            if (likely(thr_job_dequeue())) {
                continue;
            }
            if (thr_job_steal() > 0) {
                continue;
            }

            shared_write(thr0_cur_ec_g, &syn->ec);
            key = thr_ec_get(&syn->ec);
#ifndef NDEBUG
            self_g->acc.ec_gets++;
#endif
            if (!shared_read(syn->pending)) {
                goto cas;
            }
            if (!mpsc_queue_looks_empty(&thr_queue_main_g->q)) {
                goto cas;
            }

#ifndef NDEBUG
            self_g->acc.ec_waits++;
            start = hardclock();
#endif
            thr_ec_wait(&syn->ec, key);
#ifndef NDEBUG
            self_g->acc.ec_wait_time += (hardclock() - start);
#endif
          cas:
            while (unlikely(!atomic_bool_cas(&thr0_cur_ec_g, &syn->ec, NULL)))
            {
                sched_yield();
            }
        }
    } else {
        while (shared_read(syn->pending)) {
            int res = -1;
            uint64_t key = 0;

            if (likely(thr_job_dequeue())) {
                continue;
            }
            if (thr_job_steal() > 0) {
                continue;
            }
            do {
                sched_yield();
                key = thr_ec_get(&syn->ec);
#ifndef NDEBUG
                self_g->acc.ec_gets++;
#endif
            } while (shared_read(syn->pending) && (res = thr_job_steal()) < 0);
            if (res == 0) {
#ifndef NDEBUG
                self_g->acc.ec_waits++;
                start = hardclock();
#endif
                thr_ec_timedwait(&syn->ec, key, 100);
#ifndef NDEBUG
                self_g->acc.ec_wait_time += (hardclock() - start);
#endif
            }
        }
    }

    thr_syn__release(syn);
}

void thr_syn_notify(thr_syn_t *syn, thr_queue_t *q, thr_job_t *job)
{
    thr_schedule_b(^{
        thr_syn_wait(syn);
        thr_queue(q, job);
    });
}

/* }}} */
/* Tests {{{ */

#include <lib-common/z.h>

/* Wake up Thr0 {{{ */

struct {
    int count;
    uint64_t diff;
    el_t blocker;
} z_wake_up_g;

static int z_wake_up(void)
{
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);
    int iterations = 100000;

    if (fast) {
        iterations = 10000;
    }

    z_wake_up_g.blocker = el_blocker_register();
    thr_schedule_b(^{
        for (int i = 0; i < iterations; i++) {
            struct timeval start;

            assert (thr_id() != 0);
            lp_gettv(&start);
            thr_queue_sync_b(thr_queue_main_g, ^{
                struct timeval job;

                lp_gettv(&job);
                if (timeval_diff64(&job, &start) > 50000) {
                    z_wake_up_g.count++;
                }
                z_wake_up_g.diff += timeval_diff64(&job, &start);
            });
        }
        el_blocker_unregister(&z_wake_up_g.blocker);
    });
    while (z_wake_up_g.blocker) {
        el_loop_timeout(100);
    }
    Z_ASSERT_ZERO(z_wake_up_g.count);

    e_trace(3, "count: %d - %jd", z_wake_up_g.count,
            z_wake_up_g.diff / iterations);

    Z_HELPER_END;
}

/* }}} */
/* Post-Notify pattern {{{ */

struct {
    mpsc_queue_t queue;
    thr_job_t    job;
    thr_syn_t    syn;

    unsigned val __attribute__((aligned(64)));
    unsigned thr __attribute__((aligned(64)));
    unsigned mq  __attribute__((aligned(64)));
    unsigned pmq __attribute__((aligned(64)));
    bool     flushed __attribute__((aligned(64)));
} z_post_notify_g;

struct post_notify_node {
    mpsc_node_t node;
    bool flush;
} __attribute__((aligned(64)));


static void run_post_notify(thr_job_t *job, thr_syn_t *syn)
{
    mpsc_it_t it;
    bool flush = false;

    mpsc_queue_drain_start(&it, &z_post_notify_g.queue);
    do {
#define check(n)  do {                                                       \
            assert (!container_of(n, struct post_notify_node, node)->flush); \
            assert (!shared_read(z_post_notify_g.flushed));                  \
        } while (0)

        mpsc_node_t *n = mpsc_queue_drain_fast(&it, check);

        assert (!shared_read(z_post_notify_g.flushed));
        if (container_of(n, struct post_notify_node, node)->flush) {
            atomic_add(&z_post_notify_g.thr, 1);
            flush = true;
        }
    } while (!mpsc_queue_drain_end(&it, IGNORE));

    thr_syn_queue_b(&z_post_notify_g.syn, thr_queue_main_g, ^{
        if (flush) {
            z_post_notify_g.val++;
        }
        z_post_notify_g.mq++;
    });
    atomic_add(&z_post_notify_g.pmq, 1);
    if (flush) {
        assert (atomic_bool_cas(&z_post_notify_g.flushed, false, true));
    }
}

/* }}} */
/* Contention {{{ */

struct contention_job {
    thr_job_t job;
    int       i;
};

static void run_contention(thr_job_t *job_, thr_syn_t *syn)
{
    struct contention_job *job = container_of(job_, struct contention_job, job);

    job->i *= 2;
}

/* }}} */
/* Sort {{{ */

static struct {
    int  help;

    int  sort_minsize;
} z_sort_g = {
    .sort_minsize = 128,
};

struct sort_job {
    thr_job_t  job;
    uint32_t  *a;
    size_t     n;
};

static void par_qsort_run(thr_job_t *job, thr_syn_t *syn);
static void par_qsort(thr_syn_t *syn, uint32_t *a, size_t n, bool use_blocks)
{
    uint32_t qstmp, qsop2;
    uint32_t *mean, *i, *j;
    size_t n1;

#define swap(p,q)        (qstmp = *(p), *(p) = *(q), *(q) = qstmp)
#define rotate(p,r,q)    (qstmp = *(p), *(p) = *(r), *(r) = *(q), *(q) = qstmp)
#define comp_gt(a,b)     (*(a) > *(b))

    for (;;) {
        i = a;
        j = a + n - 1;

        if (comp_gt(i, j)) {
            swap(i, j);
        }
        if (n <= 2)
            return;

        mean = i + (n >> 1);
        if (comp_gt(i, mean)) {
            swap(i, mean);
        } else
        if (comp_gt(mean, j)) {
            swap(j, mean);
        }
        if (n == 3)
            return;         /* in case of 3 items */

        qsop2 = *mean;       /* cause *mean is gonna change */

        for (;;) {
            /* We do not have guards in these loops because we assume
             * compare(x, x) returns 0.  If the comparison function is not
             * regular and returns != 0 on identical arguments, all bets
             * are off and this code may crash.
             */
            while (++i, comp_gt(&qsop2, i))  /* find GE in left part */
                continue;

            while (--j, comp_gt(j, &qsop2))  /* find LE in right part */
                continue;

            if (i < j) {
                swap(i, j);        /* swap if needed */
                continue;
            }
            break;
        }

        /* i >= j : we're done and need to recurse on both ranges.
         * either i == j   : don't look at the ith element
         * or     i == j+1 : and split between them.
         */

        /* Fix ranges: [a..i[ U [j+1..a+n[ become [a..a+n[ U [j..j+n1[ */
        j++;
        n1 = a + n - j;
        n  = i - a;

        /* We want to recurse on [a..a+n[ and [j..j+n1[, but in order
         * to minimize stack usage, we only recurse on the smaller
         * range and loop on the other.
         */
        if (n <= n1) {
            SWAP(size_t, n, n1);
            SWAP(uint32_t *, a, j);
        }

        if (n1 > (size_t)z_sort_g.sort_minsize) {
            if (use_blocks) {
                thr_syn_schedule_b(syn, ^{
                    par_qsort(syn, j, n1, true);
                });
                thr_syn_schedule_b(syn, ^{
                    par_qsort(syn, a, n, true);
                });
            } else {
                struct sort_job *job;

                job = p_new_raw(struct sort_job, 1);
                job->job.run = &par_qsort_run;
                job->a    = j;
                job->n    = n1;
                thr_syn_schedule(syn, &job->job);

                job = p_new_raw(struct sort_job, 1);
                job->job.run = &par_qsort_run;
                job->a    = a;
                job->n    = n;
                thr_syn_schedule(syn, &job->job);
            }
            return;
        }

        if (n1 > 1) {
            par_qsort(syn, j, n1, use_blocks);
        }
    }
#undef intersects
#undef swap
#undef rotate
}

__flatten
static void par_qsort_run(thr_job_t *job, thr_syn_t *syn)
{
    struct sort_job *j = container_of(job, struct sort_job, job);

    par_qsort(syn, j->a, j->n, false);
    p_delete(&j);
}

static int test_qsort(bool use_blocks)
{
    t_scope;
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);
    const size_t len = fast ? (8 << 10) : (8 << 15);
    uint32_t *vec[thr_parallelism_g];
    thr_syn_t syn;

    for (size_t j = 0; j < thr_parallelism_g; j++) {
        vec[j] = p_new_raw(uint32_t, len);
        srand(0);
        for (size_t i = 0; i < len; i++) {
            vec[j][i] = (rand() << 16) | rand();
        }
    }

    z_sort_g.sort_minsize = MAX(2, z_sort_g.sort_minsize);
    thr_acc_reset();
    thr_syn_init(&syn);
    if (use_blocks) {
        for (size_t j = 0; j < thr_parallelism_g; j++) {
            uint32_t *a = vec[j];
            thr_syn_t *synp = &syn;

            thr_syn_schedule_b(&syn, ^{
                par_qsort(synp, a, len, true);
            });
        }
    } else {
        for (size_t j = 0; j < thr_parallelism_g; j++) {
            struct sort_job *job = p_new_raw(struct sort_job, 1);

            job->job.run    = &par_qsort_run;
            job->a          = vec[j];
            job->n          = len;
            thr_syn_schedule(&syn, &job->job);
        }
    }
    thr_syn_wait(&syn);

    thr_acc_trace(3, "%s", __func__);
    thr_syn_wipe(&syn);

    for (size_t j = 0; j < thr_parallelism_g; j++) {
        for (size_t i = 1; i < len; i++) {
            Z_ASSERT_LE(vec[j][i - 1], vec[j][i]);
        }
        p_delete(&vec[j]);
    }

    Z_HELPER_END;
}

/* }}} */
/* Queues {{{ */

struct test_queue {
    thr_queue_t *q;
    int i;
} __attribute__((aligned(64)));

static int test_queue(void)
{
    size_t n = thr_parallelism_g + 1;
    struct test_queue q[n];
    bool trace = e_is_traced(4);

    for (size_t i = 0; i < n; i++) {
        q[i].q = thr_queue_create();
        q[i].i = -1;
    }

    for (int i = 0; i < 20; i++) {
        for (size_t j = 0; j < n; j++) {
            struct test_queue *qj = &q[j];

            thr_queue_b(qj->q, ^{
                assert (qj->i == i - 1);
                qj->i = i;
                if (trace) {
                    fputc('1' + j, stderr);
                }
            });
        }
    }
    for (size_t j = 0; j < n; j++) {
        struct test_queue *qj = &q[j];

        thr_queue_b(qj->q, ^{
            assert (qj->i == 19);
            qj->i = 20;
            if (trace) {
                fputc('A' + j, stderr);
            }
        });
    }

    for (size_t i = 0; i < n; i++) {
        thr_queue_destroy(q[i].q, true);
        Z_ASSERT_EQ(q[i].i, 20);
    }

    if (trace) {
        fputc('\n', stderr);
    }
    Z_HELPER_END;
}

/* }}} */
/* Queue Sync {{{ */

static size_t queue_sync_g;

static int test_queue_sync(void)
{
    t_scope;
    thr_syn_t *syn = t_new_raw(thr_syn_t, 1);
    thr_queue_t *q = thr_queue_create();

    queue_sync_g = 0;
    thr_syn_init(syn);
    for (size_t i = 0; i < thr_parallelism_g; i++) {
        thr_syn_schedule_b(syn, ^{
            for (int j = 0; j < 10; j++) {
                thr_syn_schedule_b(syn, ^{
                    for (int k = 0; k < 100; k++) {
                        thr_queue_sync_b(q, ^{
                            queue_sync_g++;
                        });
                    }
                });
            }
        });
    }
    thr_syn_wait(syn);
    thr_syn_wipe(syn);
    thr_queue_destroy(q, true);

    Z_ASSERT_EQ(queue_sync_g, 10 * thr_parallelism_g * 100);
    Z_HELPER_END;
}

/* }}} */
/* Steal job {{{ */

typedef struct rci_t {
    mpsc_node_t node;
    int   seqid;
    byte *data;
} rci_t;
qvector_t(rci, rci_t *);

static struct {
    qv_t(rci) ci_ring;
    mpsc_queue_t mpsc;
    int last_acked;
} queue_steal_g = {
    .last_acked = -1,
};

static void run_node(mpsc_node_t *node, int *unqueued)
{
    usleep(1);
    (*unqueued)++;
}

static void run_job(thr_job_t *j, thr_syn_t *syn)
{
    mpsc_it_t it;
    int to_sync;
    int unqueued = 0;

    mpsc_queue_drain_start(&it, &queue_steal_g.mpsc);
    do {
        mpsc_node_t *node;
        rci_t *rci;

        node = mpsc_queue_drain_fast(&it, run_node, &unqueued);

        rci = container_of(node, rci_t, node);
        to_sync = rci->seqid;
        usleep(5);
        unqueued++;
    } while (!mpsc_queue_drain_end(&it, IGNORE));

    thr_syn_queue_b(syn, thr_queue_main_g, ^{
        while (queue_steal_g.last_acked < to_sync) {
            rci_t *rci = queue_steal_g.ci_ring.tab[0];

            queue_steal_g.last_acked++;
            assert (queue_steal_g.last_acked == rci->seqid);
            p_delete(&rci->data);
            p_delete(&rci);
            qv_skip(rci, &queue_steal_g.ci_ring, 1);
        }
    });
}

/* }}} */

Z_GROUP_EXPORT(thrjobs) {
    const bool fast = Z_HAS_MODE(FAST)
                   || mem_tool_is_running(MEM_TOOL_VALGRIND | MEM_TOOL_ASAN);

    thr_initialize();

    Z_TEST(contention, "test contention behavior") {
        struct contention_job jobs[2 * thr_parallelism_g * THR_JOB_MAX];
        int iterations = 1 << 10;

        if (fast) {
            iterations = 1 << 5;
        }

        for (int j = 0; j < iterations; j++) {
            thr_syn_t syn;

            thr_acc_reset();
            thr_syn_init(&syn);
            for (int i = 0; i < countof(jobs); i++) {
                jobs[i] = (struct contention_job) {
                    .job.run = run_contention,
                    .i       = i,
                };
                thr_syn_schedule(&syn, &jobs[i].job);
            }

            thr_syn_wait(&syn);

            for (int i = 0; i < countof(jobs); i++) {
                Z_ASSERT_EQ(jobs[i].i, i * 2);
            }

            thr_acc_trace(3, "");
            thr_syn_wipe(&syn);
        }
    } Z_TEST_END;

    Z_TEST(sort_job, "test sort with thr_job_t structure") {
        for (int i = 0; i < 16; i++) {
            z_sort_g.sort_minsize = 1 << i;
            Z_HELPER_RUN(test_qsort(false));
        }
    } Z_TEST_END;

    Z_TEST(sort_block, "test sort with blocks") {
        for (int i = 0; i < 16; i++) {
            z_sort_g.sort_minsize = 1 << i;
            Z_HELPER_RUN(test_qsort(true));
        }
    } Z_TEST_END;

    Z_TEST(queue, "test queues") {
        Z_HELPER_RUN(test_queue());
    } Z_TEST_END;

    Z_TEST(queue_syn, "tests queue sync") {
        for (int i = 0; i < (fast ? 100 : 10000); i++) {
            Z_HELPER_RUN(test_queue_sync());
        }
    } Z_TEST_END;

    Z_TEST(wake_up_thr0, "test the main thread wake up procedure") {
        Z_HELPER_RUN(z_wake_up());
    } Z_TEST_END;

    Z_TEST(post_notify, "test the post-notify pattern") {
        int iterations = 300000; /* approximatively 1s */

        mpsc_queue_init(&z_post_notify_g.queue);
        thr_syn_init(&z_post_notify_g.syn);
        z_post_notify_g.job.run = &run_post_notify;

        if (fast) {
            iterations = 50000;
        }

        for (int loops = 0; loops < iterations; loops++) {
            t_scope;
            int count = rand() % 32;
            bool posted_flush = false;
            int posted = 0;
            unsigned val = z_post_notify_g.val;
            unsigned mq  = z_post_notify_g.mq;
            struct post_notify_node *nodes = t_new(struct post_notify_node,
                                                   count + 1);

            shared_write(z_post_notify_g.flushed, false);
            for (int i = 0; i < count; i++) {
                nodes[i].flush = false;
                if (mpsc_queue_push(&z_post_notify_g.queue, &nodes[i].node)) {
                    thr_syn_schedule(&z_post_notify_g.syn, &z_post_notify_g.job);
                    posted++;
                }
            }
            nodes[count].flush = true;
            if (mpsc_queue_push(&z_post_notify_g.queue, &nodes[count].node)) {
                thr_syn_schedule(&z_post_notify_g.syn, &z_post_notify_g.job);
                posted_flush = true;
                posted++;
            }
            thr_syn_wait(&z_post_notify_g.syn);

            e_trace(3, "val=%d thr=%d => jobs=%d mq=%d pmq=%d flush=%d",
                    z_post_notify_g.val, shared_read(z_post_notify_g.thr),
                    mq + posted, z_post_notify_g.mq,
                    shared_read(z_post_notify_g.pmq), posted_flush);
            Z_ASSERT_EQ(val + 1, z_post_notify_g.val);
            Z_ASSERT_EQ(z_post_notify_g.val, shared_read(z_post_notify_g.thr));
            Z_ASSERT_EQ(shared_read(z_post_notify_g.pmq), mq + posted);
            Z_ASSERT_EQ(z_post_notify_g.mq, mq + posted);
            Z_ASSERT(shared_read(z_post_notify_g.flushed));
        }

        thr_syn_wipe(&z_post_notify_g.syn);
    } Z_TEST_END;

    Z_TEST(queue_steal, "queue steal jobs") {
        int seqid = 0;
        thr_job_t job = { .run = &run_job };
        struct timeval start;

        mpsc_queue_init(&queue_steal_g.mpsc);

        while (seqid < 30000) {
            rci_t *rci = p_new(rci_t, 1);

            rci->data  = p_new(byte, 16 << 10);
            rci->seqid = seqid++;

            qv_append(rci, &queue_steal_g.ci_ring, rci);

            if (mpsc_queue_push(&queue_steal_g.mpsc, &rci->node)) {
                thr_schedule(&job);
            }

            el_loop_timeout(queue_steal_g.ci_ring.len > 1000 ? 0 : 10);
        }

        lp_gettv(&start);
        for (;;) {
            struct timeval end;
            int64_t diff;

            lp_gettv(&end);
            diff = timeval_diffmsec(&end, &start);
            if (diff > 1000) {
                break;
            }
            el_loop_timeout(1000 - diff);
            if (queue_steal_g.last_acked == seqid - 1) {
                break;
            }
        }
        Z_ASSERT_EQ(queue_steal_g.last_acked, seqid - 1);
        Z_ASSERT_ZERO(queue_steal_g.ci_ring.len);
    } Z_TEST_END;
} Z_GROUP_END;

/* }}} */
